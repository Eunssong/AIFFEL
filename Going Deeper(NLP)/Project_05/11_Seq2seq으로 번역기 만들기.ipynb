{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0963def0",
   "metadata": {},
   "source": [
    "- 데이터 전처리\n",
    "- 모델 설계\n",
    "    - Bahdanau Attention을 사용\n",
    "- 모델 훈련\n",
    "    - Optimizer & Loss -Encoder-Decoder 구조 정의\n",
    "    - train_step 구현\n",
    "    - 학습 진행 및 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa941c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 말뭉치 사용을 위한 한국어를 지원하는 폰트로 변경\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "%config InlineBackend.figure_format = 'retina'\n",
    " \n",
    "import matplotlib.font_manager as fm\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager.findfont(font)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e2a8da",
   "metadata": {},
   "source": [
    "### 데이터 전처리\n",
    "#### 데이터 준비하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfbf1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178fed8b",
   "metadata": {},
   "source": [
    "데이터 다운로드에 tf.keras.utils.get_file() 함수 사용\n",
    "- get_file()함수는 URL로부터 데이터를 다운받고, 압축된 형식일 경우 해제해줌  \n",
    "\n",
    "\n",
    "스페인어-영어 언어 데이터 세트가 포함 된 ZIP 파일을 다운로드하고 압축을 해제 한 다음, 추출 된 내용에서 특정 텍스트 파일('spa.txt')의 경로를 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4028e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip', # 다운로드 한 zip 파일에 지정할 이름\n",
    "    origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip', #  파일이 다운로드 될 UR\n",
    "    extract=True) # 압축을 해제\n",
    "\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e060497e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 데이터 확인\n",
    "with open(path_to_file, \"r\") as f:\n",
    "    raw = f.read().splitlines()\n",
    "\n",
    "print(\"Data Size:\", len(raw))\n",
    "print(\"Example:\")\n",
    "\n",
    "for sen in raw[0:100][::20]: print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3617eac",
   "metadata": {},
   "source": [
    "데이터는 \\t 기호를 기준으로 영어와 스페인어가 병렬 쌍을 이루고 있음 \n",
    "\n",
    "\n",
    "#### 데이터 전처리: 정제하기\n",
    "- !와 같은 불필요한 기호등을 삭제함 \n",
    "- Decoder는 첫 입력으로 사용할 시작 토큰과 문장생성 종료를 알리는 끝 토큰이 반드시 필요하기 때문에 <start>, <end>를 붙여줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a16a2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence, s_token=False, e_token=False):\n",
    "    sentence = sentence.lower().strip() # 소문자 변환, 양쪽 공백 제거\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence) # 문장 내의 구두점 앞뒤에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 연속된 여러 개의 공백을 하나의 공백으로 축소\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence) #  영문자와 구두점(. ? ! ,) 이외의 모든 문자를 공백으로 대체\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    if s_token: # true\n",
    "        sentence = '<start> ' + sentence # 문장 시작 부분에 <start> 토큰을 추가\n",
    "\n",
    "    if e_token: # true\n",
    "        sentence += ' <end>' # 문장 끝 부분에 <end> 토큰을 추가\n",
    "    \n",
    "    return sentence #전처리된 문장을 반환 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a2df7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영어와 스페인어의 전처리된 문장 코퍼스(corpus)를 만드는 과정\n",
    "enc_corpus = [] # 영어\n",
    "dec_corpus = [] # 스페인어\n",
    "\n",
    "num_examples = 30000 # 데이터는 상위 3만 개만 사용\n",
    "\n",
    "for pair in raw[:num_examples]:\n",
    "    eng, spa = pair.split(\"\\t\") # 각 문장 쌍을 탭('\\t')을 기준으로 분리\n",
    "\n",
    "    enc_corpus.append(preprocess_sentence(eng)) # 영어 전처리\n",
    "    dec_corpus.append(preprocess_sentence(spa, s_token=True, e_token=True)) # 스페인어 전처리, 문장 시작과 끝에 <start>, <end> 토큰 붙임\n",
    "\n",
    "# 100번째 문장 출력해서 확인\n",
    "print(\"English:\", enc_corpus[100])   # go away !\n",
    "print(\"Spanish:\", dec_corpus[100])   # <start> salga de aqu ! <end>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acef1ccd",
   "metadata": {},
   "source": [
    "#### 데이터 전처리: 토큰화\n",
    "- 정제된 텍스트를 tokenize() 함수를 사용해 토큰화하고 텐서로 변환\n",
    "- 변환된 텐서를 80%의 훈련 데이터와 20%의 검증 데이터로 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb823aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    tensor = tokenizer.texts_to_sequences(corpus) # 텍스트를 정수 시퀀스로 변환\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post') # 패딩 적용\n",
    "\n",
    "    return tensor, tokenizer # 토큰화된 시퀀스와 학습된 토크나이저를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd365fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 훈련 데이터와 검증 데이터로 분리\n",
    "enc_tensor, enc_tokenizer = tokenize(enc_corpus)\n",
    "dec_tensor, dec_tokenizer = tokenize(dec_corpus)\n",
    "\n",
    "enc_train_tensor, enc_val_tensor, dec_train_tensor, dec_val_tensor = train_test_split(enc_tensor, dec_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# 확인을 위한 출력\n",
    "print(\"훈련 데이터의 크기:\", len(enc_train_tensor))\n",
    "print(\"검증 데이터의 크기:\", len(enc_val_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe6027c",
   "metadata": {},
   "source": [
    "### 모델 설계\n",
    "- Encoder는 모든 Time-Step의 Hidden State를 출력으로 갖고, Decoder는 Encoder의 출력과 Decoder의 t-1 Step의 Hidden State로 Attention을 취하여 t Step의 Hidden State를 만들어 냄\n",
    "- Decoder에서 t Step의 단어로 예측된 것을 실제 정답과 대조해 Loss를 구하고, 생성된 t Step의 Hidden State는 t+1 Step의 Hidden State를 만들기 위해 다시 Decoder에 전달함\n",
    "- Bahdanau Attention : 입력 시퀀스의 각 위치마다 가중치를 학습하여 중요도를 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cc7f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.w_dec = tf.keras.layers.Dense(units)# 디코더의 hidden state를 처리\n",
    "        self.w_enc = tf.keras.layers.Dense(units)# 인코더의 hidden state를 처리\n",
    "        self.w_com = tf.keras.layers.Dense(1) # 어텐션 가중치를 결합하는 데 사용되는 밀집 레이어\n",
    "    \n",
    "    def call(self, h_enc, h_dec):\n",
    "        # h_enc shape: [batch x length x units]\n",
    "        # h_dec shape: [batch x units]\n",
    "\n",
    "        h_enc = self.w_enc(h_enc) # 인코더 hidden state에 대한 가중치를 계산\n",
    "        h_dec = tf.expand_dims(h_dec, 1) # 디코더 hidden state에 차원을 추가하여 broadcasting이 가능하도록 함\n",
    "        h_dec = self.w_dec(h_dec) # 디코더 hidden state에 대한 가중치를 계산\n",
    "\n",
    "        score = self.w_com(tf.nn.tanh(h_dec + h_enc)) # 어텐션 스코어를 계산\n",
    "        #  디코더 hidden state와 인코더 hidden state 간의 유사도를 나타냄\n",
    "        \n",
    "        attn = tf.nn.softmax(score, axis=1) # 각 인코더의 hidden state에 대한 가중치가 얻어냄\n",
    "  \n",
    "        context_vec = attn * h_enc # 어텐션 가중치를 사용하여 인코더 hidden state를 가중 평균\n",
    "        context_vec = tf.reduce_sum(context_vec, axis=1) # 시퀀스 길이에 따라 가중 평균을 합산하여 컨텍스트 벡터를 계산\n",
    "\n",
    "        return context_vec, attn # 계산된 컨텍스트 벡터와 어텐션 가중치를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f8cbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    # vocab_size: 어휘 사전의 크기, 즉 모델이 다루는 고유 단어의 수\n",
    "    # embedding_dim: 임베딩 차원, 즉 각 단어를 나타내는 벡터의 크기\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.enc_units = enc_units # enc_units: GRU 레이어의 유닛 수, 즉 GRU 레이어의 출력 차원\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) # 주어진 단어 인덱스를 임베딩 벡터로 변환\n",
    "        self.gru = tf.keras.layers.GRU(enc_units,\n",
    "                                       return_sequences=True) # 모든 시점의 출력을 반환하도록 설정\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.gru(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a27eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        # 출력 레이어로, 디코더의 출력을 어휘 크기에 맞게 변환\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # Bahdanau 어텐션 메커니즘을 사용하는 어텐션 레이어를 생성\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, h_dec, enc_out):\n",
    "        # h_dec : 디코더의 이전 시점의 hidden state\n",
    "        # enc_out: 인코더의 출력으로, 모든 시점에서의 인코더의 hidden state\n",
    "        \n",
    "        #  Bahdanau 어텐션 레이어를 사용하여 현재 디코더의 hidden state와 인코더의 hidden state 간의 어텐션 가중치 및 컨텍스트 벡터를 계산\n",
    "        context_vec, attn = self.attention(enc_out, h_dec) \n",
    "\n",
    "        # 임베딩 및 어텐션 가중치 추가\n",
    "        out = self.embedding(x)\n",
    "        # 어텐션 가중치를 현재 입력에 추가\n",
    "        out = tf.concat([tf.expand_dims(context_vec, 1), out], axis=-1)\n",
    "        \n",
    "        # GRU 레이어 실행\n",
    "        #어텐션을 추가한 입력을 GRU 레이어에 전달하여 디코더의 현재 시점의 출력과 hidden state를 얻음\n",
    "        out, h_dec = self.gru(out)\n",
    "        \n",
    "        # 출력 레이어 젹용\n",
    "        out = tf.reshape(out, (-1, out.shape[2]))\n",
    "        out = self.fc(out) # 최종 출력 생성\n",
    "\n",
    "        return out, h_dec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cd47e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 정의\n",
    "BATCH_SIZE     = 64\n",
    "SRC_VOCAB_SIZE = len(enc_tokenizer.index_word) + 1\n",
    "TGT_VOCAB_SIZE = len(dec_tokenizer.index_word) + 1\n",
    "\n",
    "units         = 1024\n",
    "embedding_dim = 512\n",
    "\n",
    "# 인코더 및 디코더 모델 초기화\n",
    "encoder = Encoder(SRC_VOCAB_SIZE, embedding_dim, units)\n",
    "decoder = Decoder(TGT_VOCAB_SIZE, embedding_dim, units)\n",
    "\n",
    "# 샘플 입력 및 인코더 출력 확인\n",
    "sequence_len = 30\n",
    "\n",
    "sample_enc = tf.random.uniform((BATCH_SIZE, sequence_len)) # 랜덤한 인코더 입력 시퀀스를 생성\n",
    "sample_output = encoder(sample_enc) # 인코더에 랜덤한 입력을 전달하여 인코더 출력을 확인\n",
    "\n",
    "print ('Encoder Output:', sample_output.shape)\n",
    "\n",
    "# 샘플 디코더 출력 및 어텐션 확인\n",
    "sample_state = tf.random.uniform((BATCH_SIZE, units))\n",
    "\n",
    "# 디코더에 랜덤한 입력과 hidden state를 전달하여 디코더 출력 및 어텐션 가중치를 확인\n",
    "sample_logits, h_dec, attn = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                     sample_state, sample_output)\n",
    "\n",
    "print ('Decoder Output:', sample_logits.shape)\n",
    "print ('Decoder Hidden State:', h_dec.shape)\n",
    "print ('Attention:', attn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207f87c3",
   "metadata": {},
   "source": [
    "### 모델 훈련\n",
    "\n",
    "#### Optimizer & Loss\n",
    "- tf.math.logical_not(tf.math.equal(real, 0))는 TensorFlow에서 사용되는 텐서 연산으로, 패딩된 부분을 제외한 유효한 데이터를 나타내는 마스크를 생성하는 역할을 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87061333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 옵티마이저 및 손실함수 초기화\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none') # 신경망 출력이 로짓인 경우에 사용\n",
    "# reduction='none'은 손실을 각 샘플에 대해 계산하고, 나중에 모든 샘플에 대한 평균을 계산할 때 사용\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    # 패딩된 부분은 고려하지 않도록 하기 위해 패딩된 부분에 대한 마스크를 생성\n",
    "    # 패딩은 정수 0으로 표현되고, 이를 기준으로 마스크를 생성\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    \n",
    "    # 실제 레이블과 모델의 예측값 간의 손실을 계산\n",
    "    loss = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss.dtype) # 마스크를 손실 데이터 형식으로 변환\n",
    "    loss *= mask # 손실에 마스크를 곱하여 패딩된 부분의 손실을 0으로 만듬\n",
    "    \n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f36feda",
   "metadata": {},
   "source": [
    "#### train_step 구현\n",
    "- train_step()은 학습에 필요한 것을 모두 가져가 Loss를 계산한 후 반환하는 함수\n",
    "- 학습 과정\n",
    "\n",
    "    1) Encoder에 소스 문장을 전달해 컨텍스트 벡터인 enc_out 을 생성  \n",
    "    \n",
    "    2) t=0일 때, Decoder의 Hidden State는 Encoder의 Final State로 정의. h_dec = enc_out[:, -1]  \n",
    "    \n",
    "    3) Decoder에 입력으로 전달할 \\<start> 토큰 문장 생성  \n",
    "    \n",
    "    4) \\<start> 문장과 enc_out, Hidden State를 기반으로 다음 단어(t=1)를 예측. pred  \n",
    "    \n",
    "    5) 예측된 단어와 정답 간의 Loss를 구한 후, t=1의 정답 단어를 다음 입력으로 사용 (예측 단어 X)  \n",
    "    \n",
    "    6) 반복!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26a8735",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "\n",
    "# src: 입력 소스 문장의 텐서\n",
    "# tgt: 목표 타겟 문장의 텐서\n",
    "# dec_tok: 디코더의 토크나이저 객체\n",
    "def train_step(src, tgt, encoder, decoder, optimizer, dec_tok):\n",
    "    bsz = src.shape[0]\n",
    "    loss = 0\n",
    "\n",
    "    # 그래디언트 계산\n",
    "    with tf.GradientTape() as tape: \n",
    "        enc_out = encoder(src) #  주어진 소스 문장에 대해 인코더를 실행하여 인코더 출력을 얻음\n",
    "        h_dec = enc_out[:, -1] # 디코더의 초기 hidden state를 인코더 출력의 마지막 시점으로 설정\n",
    "        \n",
    "        dec_src = tf.expand_dims([dec_tok.word_index['<start>']] * bsz, 1) # start 토큰 추가\n",
    "\n",
    "        # 디코딩 루프 : 디코더를 반복적으로 실행해서 예측을 생성하고 손실을 계산\n",
    "        for t in range(1, tgt.shape[1]):\n",
    "            pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n",
    "\n",
    "            loss += loss_function(tgt[:, t], pred) # 각 시점에서의 손실을 누적\n",
    "            \n",
    "            # 다음 시점의 디코더 입력으로 현재 타임 스텝의 실제 값을 사용\n",
    "            dec_src = tf.expand_dims(tgt[:, t], 1)\n",
    "        \n",
    "    batch_loss = (loss / int(tgt.shape[1])) # 전체 손실을 타임 스텝 수로 나누어 배치 손실 계산\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables # 훈련 가능한 변수 가져옴\n",
    "    gradients = tape.gradient(loss, variables) # 그래디언트를 계산\n",
    "    optimizer.apply_gradients(zip(gradients, variables)) # 옵티마이저를 사용해서 모델의 기울기를 업데이트\n",
    "    \n",
    "    return batch_loss # 배치 손실 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc964b08",
   "metadata": {},
   "source": [
    "이러한 훈련 스텝 함수는 Seq2Seq 모델의 훈련 과정 중 한 스텝을 수행하며, 주어진 데이터 배치에 대해 손실을 계산하고 모델을 업데이트함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ddde68",
   "metadata": {},
   "source": [
    "#### 훈련 시작하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f80da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm    # 작업 진행 상태 모니터링\n",
    "import random\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    # : 인덱스 리스트를 생성하여 데이터셋을 미니배치로 나누기 위한 인덱스를 미리 계산\n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE)) # 각 배치의 시작 인덱스\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)    # tqdm\n",
    "    \n",
    "    # 미니 배치 순회\n",
    "    for (batch, idx) in enumerate(t): \n",
    "        batch_loss = train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                                dec_train[idx:idx+BATCH_SIZE],\n",
    "                                encoder,\n",
    "                                decoder,\n",
    "                                optimizer,\n",
    "                                dec_tokenizer)\n",
    "    \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        # tqdm에 현재 에포크 번호를 표시\n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))    # tqdm\n",
    "        # 현재까지의 평균 손실을 표시\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))    # tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e8fe15",
   "metadata": {},
   "source": [
    "이러한 훈련 루프를 통해 모델은 주어진 에포크 동안 전체 훈련 데이터셋에 대해 반복적으로 학습되며, tqdm을 사용하여 진행 상태를 모니터링함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21da3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Set을 사용하는 eval_step() 함수를 정의\n",
    "# # eval_step() 정의하기\n",
    "# train_step() 이후 eval_step() 진행하도록 소스 수정하기\n",
    "\n",
    "@tf.function\n",
    "def eval_step(src, tgt, encoder, decoder, dec_tok):\n",
    "    bsz = src.shape[0]\n",
    "    loss = 0\n",
    "\n",
    "    enc_out = encoder(src)\n",
    "\n",
    "    h_dec = enc_out[:, -1]\n",
    "    \n",
    "    dec_src = tf.expand_dims([dec_tok.word_index['<start>']] * bsz, 1)\n",
    "\n",
    "    for t in range(1, tgt.shape[1]):\n",
    "        pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n",
    "\n",
    "        loss += loss_function(tgt[:, t], pred)\n",
    "        dec_src = tf.expand_dims(tgt[:, t], 1)\n",
    "        \n",
    "    batch_loss = (loss / int(tgt.shape[1]))\n",
    "    \n",
    "    return batch_loss\n",
    "\n",
    "\n",
    "# Training Process\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss = eval_step(enc_train[idx:idx+BATCH_SIZE], # 데이터셋이 evaluation을 위한 데이터셋으로 변경되어야 함\n",
    "                                dec_train[idx:idx+BATCH_SIZE],\n",
    "                                encoder,\n",
    "                                decoder,\n",
    "                                optimizer,\n",
    "                                dec_tokenizer)\n",
    "    \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "    \n",
    "    test_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, enc_val.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)\n",
    "\n",
    "    for (test_batch, idx) in enumerate(t):\n",
    "        test_batch_loss = eval_step(enc_val[idx:idx+BATCH_SIZE],\n",
    "                                    dec_val[idx:idx+BATCH_SIZE],\n",
    "                                    encoder,\n",
    "                                    decoder,\n",
    "                                    dec_tokenizer)\n",
    "    \n",
    "        test_loss += test_batch_loss\n",
    "\n",
    "        t.set_description_str('Test Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Test Loss %.4f' % (test_loss.numpy() / (test_batch + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7598a303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, encoder, decoder):\n",
    "    attention = np.zeros((dec_train.shape[-1], enc_train.shape[-1]))\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = enc_tokenizer.texts_to_sequences([sentence.split()])\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    enc_out = encoder(inputs)\n",
    "\n",
    "    dec_hidden = enc_out[:, -1]\n",
    "    dec_input = tf.expand_dims([dec_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(dec_train.shape[-1]):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0]).numpy()\n",
    "\n",
    "        result += dec_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if dec_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention\n",
    "\n",
    "\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def translate(sentence, encoder, decoder):\n",
    "    result, sentence, attention = evaluate(sentence, encoder, decoder)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention = attention[:len(result.split()), :len(sentence.split())]\n",
    "    plot_attention(attention, sentence.split(), result.split(' '))\n",
    "\n",
    "\n",
    "translate(\"Can I have some coffee?\", encoder, decoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
