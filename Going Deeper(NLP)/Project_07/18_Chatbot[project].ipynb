{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe07e0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.21.4\n",
      "1.3.3\n",
      "2.6.0\n",
      "3.6.5\n",
      "3.8.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "print(np.__version__)\n",
    "print(pd.__version__)\n",
    "print(tf.__version__)\n",
    "print(nltk.__version__)\n",
    "print(gensim.__version__) # 원래 4.1.2였음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc5e6f1",
   "metadata": {},
   "source": [
    "# ChatbotData 다운로드\n",
    "[ChatbotData](https://github.com/songys/Chatbot_data/blob/master/ChatbotData.csv)\n",
    "\n",
    "- 11,876개의 문답 페어로 이루어진 데이터\n",
    "- 일상다반사 0, 이별(부정) 1, 사랑(긍정) 2로 레이블링되어 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c49a35c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-23 13:32:05--  https://github.com/songys/Chatbot_data/raw/master/ChatbotData.csv\n",
      "Resolving github.com (github.com)... 192.30.255.113\n",
      "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv [following]\n",
      "--2024-01-23 13:32:05--  https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 889842 (869K) [text/plain]\n",
      "Saving to: ‘ChatbotData.csv.2’\n",
      "\n",
      "ChatbotData.csv.2   100%[===================>] 868.99K  --.-KB/s    in 0.04s   \n",
      "\n",
      "2024-01-23 13:32:05 (20.6 MB/s) - ‘ChatbotData.csv.2’ saved [889842/889842]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/songys/Chatbot_data/raw/master/ChatbotData.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02a5001e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChatbotData = pd.read_csv('./ChatbotData.csv')\n",
    "ChatbotData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca711b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = ChatbotData['Q']\n",
    "answers = ChatbotData['A']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ef2fdf",
   "metadata": {},
   "source": [
    "# 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fe775ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, 안녕하세요! 123 this is a test sentence... preprocessing\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    # 영문자 소문자로 변환\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # 정규식을 사용하여 영문자, 한글, 숫자, 주요 특수문자 제외 모든 문자 제거\n",
    "    sentence = re.sub(r\"[^a-zA-Z가-힣0-9.,!?]\", \" \", sentence)\n",
    "    \n",
    "    # 여러 공백을 하나의 공백으로 축소\n",
    "    sentence = re.sub(r\"\\s+\", \" \", sentence)\n",
    "    \n",
    "    return sentence.strip()\n",
    "\n",
    "# 테스트\n",
    "example_sentence = \"Hello, 안녕하세요!      123 This is a test sentence... #preprocessing\"\n",
    "processed_sentence = preprocess_sentence(example_sentence)\n",
    "print(processed_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f62f77",
   "metadata": {},
   "source": [
    "# 데이터 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebf0a19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "소스 문장 예시: [['12', '시', '땡', '!'], ['1', '지망', '학교', '떨어졌', '어'], ['3', '박', '4', '일', '놀', '러', '가', '고', '싶', '다'], ['3', '박', '4', '일', '정도', '놀', '러', '가', '고', '싶', '다'], ['ppl', '심하', '네']]\n",
      "타겟 문장 예시: [['하루', '가', '또', '가', '네요', '.'], ['위로', '해', '드립니다', '.'], ['여행', '은', '언제나', '좋', '죠', '.'], ['여행', '은', '언제나', '좋', '죠', '.'], ['눈살', '이', '찌푸려', '지', '죠', '.']]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "\n",
    "\n",
    "def build_corpus(src_sentences, tgt_sentences, tokenize_fn, threshold=20):\n",
    "    src_corpus, tgt_corpus = [], []\n",
    "    seen_pairs = set()\n",
    "\n",
    "    for src, tgt in zip(src_sentences, tgt_sentences):\n",
    "        # 정제 및 토큰화\n",
    "        src = preprocess_sentence(src)\n",
    "        tgt = preprocess_sentence(tgt)\n",
    "        src_tokens = tokenize_fn(src)\n",
    "        tgt_tokens = tokenize_fn(tgt)\n",
    "\n",
    "        # 일정 길이 이상인 문장 및 중복된 문장 제외\n",
    "        if len(src_tokens) <= threshold and len(tgt_tokens) <= threshold:\n",
    "            pair = (src, tgt)\n",
    "            if pair not in seen_pairs:\n",
    "                src_corpus.append(src_tokens)\n",
    "                tgt_corpus.append(tgt_tokens)\n",
    "                seen_pairs.add(pair)\n",
    "\n",
    "    return src_corpus, tgt_corpus\n",
    "\n",
    "# 토큰화에는 mecab.morphs 함수를 사용\n",
    "que_corpus, ans_corpus = build_corpus(questions, answers, mecab.morphs)\n",
    "\n",
    "# 결과 출력 (예시로 상위 5개 출력)\n",
    "print(\"소스 문장 예시:\", que_corpus[:5])\n",
    "print(\"타겟 문장 예시:\", ans_corpus[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45489e3c",
   "metadata": {},
   "source": [
    "# Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaf8c0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /opt/conda/lib/python3.9/site-packages (5.0.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from gdown) (3.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.9/site-packages (from gdown) (4.6.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from gdown) (4.62.3)\n",
      "Requirement already satisfied: requests[socks] in /opt/conda/lib/python3.9/site-packages (from gdown) (2.26.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests[socks]->gdown) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests[socks]->gdown) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests[socks]->gdown) (2.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests[socks]->gdown) (2.0.8)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.9/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f6f2ab0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=0B0ZXk88koS2KbDhXdWg1Q2RydlU\n",
      "From (redirected): https://drive.google.com/uc?id=0B0ZXk88koS2KbDhXdWg1Q2RydlU&confirm=t&uuid=69623ebc-e5f3-4eb4-a425-a50c1265e03e\n",
      "To: /aiffel/aiffel/DeepGoing_NLP/ko.zip\n",
      "100%|██████████| 80.6M/80.6M [00:01<00:00, 67.0MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/aiffel/aiffel/DeepGoing_NLP/ko.zip'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown\n",
    "\n",
    "# 다운로드 받을 파일의 Google Drive 공유 링크\n",
    "google_drive_link = \"https://drive.google.com/uc?id=0B0ZXk88koS2KbDhXdWg1Q2RydlU\"\n",
    "\n",
    "# 다운로드 받을 파일의 이름\n",
    "output_filename = \"ko.zip\"\n",
    "\n",
    "# 다운로드 받을 경로\n",
    "output_path = os.path.join(\"/aiffel/aiffel/DeepGoing_NLP\", output_filename)\n",
    "\n",
    "# gdown을 사용하여 Google Drive에서 파일 다운로드\n",
    "gdown.download(google_drive_link, output_path, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d20c6a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "zip_path = \"/aiffel/aiffel/DeepGoing_NLP/ko.zip\"\n",
    "extracted_path = \"/aiffel/aiffel/DeepGoing_NLP/\"\n",
    "\n",
    "# Extract the contents of the ZIP file\n",
    "with ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extracted_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2670cd6f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim==3.8.3 in /opt/conda/lib/python3.9/site-packages (3.8.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (5.2.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (1.21.4)\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 호환문제로 에러가 발생하여 다운그레이드를 진행\n",
    "'''\n",
    "Can't get attribute 'Vocab' on <module 'gensim.models.word2vec' from '/opt/conda/lib/python3.9/site-packages/gensim/models/word2vec.py'>\n",
    "'''\n",
    "!pip install --upgrade gensim==3.8.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8188f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('낱말', 0.8457916975021362), ('용어', 0.7469839453697205), ('어휘', 0.7309367656707764), ('접미사', 0.6993076801300049), ('접두사', 0.6960497498512268)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# 다운로드 받은 ko.bin 파일의 경로를 적절히 설정\n",
    "model_path = \"./ko.bin\"\n",
    "\n",
    "# Word2Vec 모델 로드\n",
    "model = Word2Vec.load(model_path)\n",
    "\n",
    "# 모델 사용 예시\n",
    "similar_words = model.wv.most_similar('단어', topn=5)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bd05240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def lexical_sub(sentence, embedding_model, top_n=5):\n",
    "    \"\"\"\n",
    "    주어진 문장에 대해 단어 임베딩을 기반으로 한 어휘 대체를 수행합니다.\n",
    "\n",
    "    Args:\n",
    "    - sentence (str): 어휘 대체를 수행할 입력 문장입니다.\n",
    "    - embedding_model: 사전 훈련된 단어 임베딩 모델 (예: Word2Vec, FastText).\n",
    "    - top_n (int, optional): 각 단어를 대체할 상위 후보의 수 (기본값은 5).\n",
    "\n",
    "    Returns:\n",
    "    - str: 어휘 대체 후의 증강된 문장입니다.\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    augmented_sentence = []\n",
    "\n",
    "    for word in words:\n",
    "        if word in embedding_model:\n",
    "            word_embedding = np.array([embedding_model[word]])\n",
    "            word_similarities = embedding_model.similar_by_vector(word_embedding.flatten(), topn=top_n)\n",
    "            top_candidates = [candidate for candidate, _ in word_similarities]\n",
    "\n",
    "            # 가장 유사한 후보로 단어를 대체합니다.\n",
    "            augmented_sentence.append(random.choice(top_candidates))\n",
    "        else:\n",
    "            augmented_sentence.append(word)\n",
    "\n",
    "    return ' '.join(augmented_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f11a8a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 문장: ['12', '시', '땡', '!']\n",
      "Augmentation된 문장: ['12', '시의', '경음', '짱']\n",
      "원본 데이터 크기: 11599\n",
      "Augmentation된 데이터 크기: 23198\n"
     ]
    }
   ],
   "source": [
    "# Augmentation 수행 및 데이터 확장\n",
    "augmented_que_corpus = []\n",
    "augmented_ans_corpus = []\n",
    "\n",
    "# 원본 데이터에 대해 Augmentation 수행\n",
    "for que_list, ans_list in zip(que_corpus, ans_corpus):\n",
    "    augmented_que_list = [lexical_sub(que, model.wv) for que in que_list]\n",
    "    augmented_ans_list = [lexical_sub(ans, model.wv) for ans in ans_list]\n",
    "    \n",
    "    augmented_que_corpus.append(augmented_que_list)\n",
    "    augmented_ans_corpus.append(augmented_ans_list)\n",
    "\n",
    "# 반대로도 수행하여 데이터 3배로 늘리기\n",
    "for que_list, ans_list in zip(que_corpus, ans_corpus):\n",
    "    augmented_que_list = [lexical_sub(ans, model.wv) for ans in ans_list]\n",
    "    augmented_ans_list = [lexical_sub(que, model.wv) for que in que_list]\n",
    "    \n",
    "    augmented_que_corpus.append(augmented_que_list)\n",
    "    augmented_ans_corpus.append(augmented_ans_list)\n",
    "\n",
    "# 결과 확인\n",
    "print(\"원본 문장:\", que_corpus[0])\n",
    "print(\"Augmentation된 문장:\", augmented_que_corpus[0])\n",
    "\n",
    "# 데이터의 크기 확인\n",
    "print(\"원본 데이터 크기:\", len(que_corpus))\n",
    "print(\"Augmentation된 데이터 크기:\", len(augmented_que_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59a4c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# 파일로 저장할 경로 지정\n",
    "save_path = \"/path/to/save/augmented_data.csv\"\n",
    "\n",
    "# Augmentation된 데이터를 행 단위로 CSV 파일에 저장\n",
    "with open(save_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    \n",
    "    # 헤더 작성\n",
    "    csvwriter.writerow([\"source\", \"target\"])\n",
    "    \n",
    "    # Augmentation된 데이터 작성\n",
    "    for augmented_que_list, augmented_ans_list in zip(augmented_que_corpus, augmented_ans_corpus):\n",
    "        csvwriter.writerow([\" \".join(augmented_que_list), \" \".join(augmented_ans_list)])\n",
    "\n",
    "print(\"Augmentation된 데이터가 성공적으로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e91586",
   "metadata": {},
   "source": [
    "# 데이터 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "861460a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start>', '하루', '가', '또', '가', '네요', '.', '<end>']\n"
     ]
    }
   ],
   "source": [
    "# 타겟 데이터에 <start> 토큰과 <end> 토큰 추가\n",
    "ans_corpus_with_tokens = [[\"<start>\"] + ans + [\"<end>\"] for ans in ans_corpus]\n",
    "\n",
    "# 결과 확인\n",
    "print(ans_corpus_with_tokens[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea974121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 사전 크기: 6785\n",
      "소스 데이터 (enc_train) 크기: (11599, 20)\n",
      "타겟 데이터 (dec_train) 크기: (11599, 22)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 소스 데이터와 타겟 데이터 결합\n",
    "combined_corpus = que_corpus + ans_corpus_with_tokens  # ans_corpus_with_tokens는 <start>, <end> 토큰 추가된 데이터\n",
    "\n",
    "# 토크나이저 생성 및 단어 사전 구축\n",
    "tokenizer = Tokenizer(filters='', oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(combined_corpus)\n",
    "\n",
    "# 단어 사전 크기 확인\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"단어 사전 크기:\", vocab_size)\n",
    "\n",
    "# 소스 데이터와 타겟 데이터를 정수 시퀀스로 변환\n",
    "enc_train = tokenizer.texts_to_sequences(que_corpus)\n",
    "dec_train = tokenizer.texts_to_sequences(ans_corpus_with_tokens)\n",
    "\n",
    "# 패딩 작업\n",
    "enc_train = pad_sequences(enc_train, padding='post')\n",
    "dec_train = pad_sequences(dec_train, padding='post')\n",
    "\n",
    "# 결과 확인\n",
    "print(\"소스 데이터 (enc_train) 크기:\", enc_train.shape)\n",
    "print(\"타겟 데이터 (dec_train) 크기:\", dec_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cf5403",
   "metadata": {},
   "source": [
    "# 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad4745ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    # 각 포지션과 차원에 대한 각도를 계산하는 함수\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, (2*(i//2)) / np.float32(d_model))\n",
    "\n",
    "    # 각 포지션에 대한 각도 벡터를 생성하는 함수\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    # 각 포지션과 차원에 대한 사인과 코사인 값을 가지는 테이블을 생성\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    # 짝수 차원에는 사인 값을, 홀수 차원에는 코사인 값을 적용\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # 짝수 차원\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # 홀수 차원\n",
    "\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bef9be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    # 입력 시퀀스에서 패딩(0)에 해당하는 위치를 찾아 마스크 생성\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_lookahead_mask(size):\n",
    "    # 룩어헤드 마스크 생성: 현재 위치 이후의 위치에 대해 마스킹\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    # 인코더 입력 패딩 마스크 생성\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "\n",
    "    # 디코더-인코더 어텐션 마스크 생성 (인코더 입력의 패딩 위치 마스킹)\n",
    "    dec_enc_mask = generate_padding_mask(src)\n",
    "\n",
    "    # 디코더 마스크 생성 (패딩 위치와 룩어헤드 마스킹을 포함)\n",
    "    dec_lookahead_mask = generate_lookahead_mask(tgt.shape[1])\n",
    "    dec_tgt_padding_mask = generate_padding_mask(tgt)\n",
    "    dec_mask = tf.maximum(dec_tgt_padding_mask, dec_lookahead_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce634641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        # 각각의 선형 변환을 수행하는 레이어들을 정의\n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        # 마지막에 사용되는 선형 변환 레이어\n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        # 스케일링된 점곱 어텐션을 수행하는 메소드\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        # 마스크 적용 (필요한 경우)\n",
    "        if mask is not None:\n",
    "            scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        # 소프트맥스 함수를 통해 어텐션 가중치 계산\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        # 가중 평균을 통해 출력 계산\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        # 주어진 텐서를 여러 헤드로 나누는 메소드\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        # 나누어진 헤드를 결합하는 메소드\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "    \n",
    "    def call(self, Q, K, V, mask):\n",
    "        # 주어진 Q, K, V에 대한 멀티헤드 어텐션을 수행하는 메소드\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        # 각각의 헤드로 나누기\n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "        \n",
    "        # 스케일드 닷 프로덕트 어텐션 수행\n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "                        \n",
    "        # 헤드 결합\n",
    "        out = self.combine_heads(out)\n",
    "        # 최종 선형 변환 수행\n",
    "        out = self.linear(out)\n",
    "            \n",
    "        return out, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdd393a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        # 첫 번째 fully-connected layer (linear layer) - 활성화 함수는 ReLU\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        # 두 번째 fully-connected layer (linear layer)\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        # 주어진 입력을 첫 번째 fully-connected layer에 전달하고 ReLU 활성화 함수를 적용\n",
    "        out = self.fc1(x)\n",
    "        # 결과를 두 번째 fully-connected layer에 전달\n",
    "        out = self.fc2(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97c70d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        # 멀티헤드 어텐션 레이어\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        # 위치별 피드포워드 네트워크 레이어\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        # 각 레이어마다의 레이어 정규화 레이어\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        # 드롭아웃 레이어\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        # Residual Connection을 위한 원본 입력 저장\n",
    "        residual = x\n",
    "        # 첫 번째 레이어 정규화\n",
    "        out = self.norm_1(x)\n",
    "        # 멀티헤드 어텐션 레이어 수행\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        # 드롭아웃 적용\n",
    "        out = self.do(out)\n",
    "        # Residual Connection 수행\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        # Residual Connection을 위한 원본 입력 저장\n",
    "        residual = out\n",
    "        # 두 번째 레이어 정규화\n",
    "        out = self.norm_2(out)\n",
    "        # 위치별 피드포워드 네트워크 레이어 수행\n",
    "        out = self.ffn(out)\n",
    "        # 드롭아웃 적용\n",
    "        out = self.do(out)\n",
    "        # Residual Connection 수행\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7a496b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        # 디코더 자체 어텐션 레이어\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        # 디코더-인코더 어텐션 레이어\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        # 위치별 피드포워드 네트워크 레이어\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        # 각 레이어마다의 레이어 정규화 레이어\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        # 드롭아웃 레이어\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        '''\n",
    "        Masked Multi-Head Attention\n",
    "        '''\n",
    "        # Residual Connection을 위한 원본 입력 저장\n",
    "        residual = x\n",
    "        # 첫 번째 레이어 정규화\n",
    "        out = self.norm_1(x)\n",
    "        # 디코더 자체 어텐션 레이어 수행\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        # 드롭아웃 적용\n",
    "        out = self.do(out)\n",
    "        # Residual Connection 수행\n",
    "        out += residual\n",
    "\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        # Residual Connection을 위한 원본 입력 저장\n",
    "        residual = out\n",
    "        # 두 번째 레이어 정규화\n",
    "        out = self.norm_2(out)\n",
    "        # 디코더-인코더 어텐션 레이어 수행\n",
    "        # Q, K, V 순서에 주의하세요!\n",
    "        out, dec_enc_attn = self.enc_dec_attn(Q=out, K=enc_out, V=enc_out, mask=dec_enc_mask)\n",
    "        # 드롭아웃 적용\n",
    "        out = self.do(out)\n",
    "        # Residual Connection 수행\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        # Residual Connection을 위한 원본 입력 저장\n",
    "        residual = out\n",
    "        # 세 번째 레이어 정규화\n",
    "        out = self.norm_3(out)\n",
    "        # 위치별 피드포워드 네트워크 레이어 수행\n",
    "        out = self.ffn(out)\n",
    "        # 드롭아웃 적용\n",
    "        out = self.do(out)\n",
    "        # Residual Connection 수행\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98e07d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # 인코더 레이어를 n_layers 개수만큼 생성\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                           for _ in range(n_layers)]\n",
    "        \n",
    "        # 드롭아웃 레이어\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        # 입력 데이터 초기화\n",
    "        out = x\n",
    "    \n",
    "        # 각 인코더 레이어를 순회하면서 호출\n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            # 인코더 레이어 호출\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            # 어텐션 가중치 저장\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3614d3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # 디코더 레이어를 n_layers 개수만큼 생성\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                           for _ in range(n_layers)]\n",
    "                            \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        # 입력 데이터 초기화\n",
    "        out = x\n",
    "    \n",
    "        # 각 디코더 레이어를 순회하면서 호출\n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            # 디코더 레이어 호출\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "                self.dec_layers[i](out, enc_out, dec_enc_mask, padding_mask)\n",
    "\n",
    "            # 각 레이어에서 얻은 어텐션 가중치 저장\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        # 최종 디코더 레이어의 출력과 각 레이어에서의 어텐션 가중치 리스트 반환\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ce87242",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, src_vocab_size, tgt_vocab_size, pos_len, dropout=0.2, shared_fc=True, shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # 입력으로 받은 d_model을 float32 타입으로 변경\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        # 공유 임베딩 여부에 따라 인코더와 디코더의 임베딩 레이어 생성\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        # 위치 인코딩 레이어 생성\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        # 인코더와 디코더 생성\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        # 출력 레이어 생성\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        # 공유 출력 레이어 여부에 따라 가중치 공유\n",
    "        self.shared_fc = shared_fc\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        # 입력에 임베딩 레이어 적용\n",
    "        out = emb(x)\n",
    "\n",
    "        # 공유된 출력 레이어를 사용할 경우 sqrt(d_model)로 스케일 조정\n",
    "        if self.shared_fc:\n",
    "            out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        # 위치 인코딩 추가\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def call(self, enc_in, dec_in, enc_mask, dec_enc_mask, dec_mask):\n",
    "        # 인코더 입력과 디코더 입력에 임베딩 적용\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        # 인코더와 디코더 순전파\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        dec_out, dec_attns, dec_enc_attns = self.decoder(dec_in, enc_out, dec_enc_mask, dec_mask)\n",
    "\n",
    "        # 출력 레이어를 통과한 최종 로짓 반환\n",
    "        logits = self.fc(dec_out)\n",
    "\n",
    "        # 로짓, 인코더 어텐션, 디코더 어텐션, 디코더-인코더 어텐션 반환\n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fdccc687",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=vocab_size,\n",
    "    tgt_vocab_size=vocab_size,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\n",
    "d_model = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "406b33a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        \n",
    "        # 모델의 임베딩 차원과 워머핑 스텝 수 초기화\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        # 학습률을 조절하기 위한 함수 정의\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        # 두 가지 학습률 조절 방법 중 작은 값 선택\n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b37a5934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LearningRateScheduler를 사용하여 학습률 스케줄링 설정\n",
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "# Adam 옵티마이저 설정\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate,      # 학습률은 LearningRateScheduler에 의해 동적으로 결정됨\n",
    "    beta_1=0.9,          # Adam 옵티마이저의 하이퍼파라미터 beta_1 설정\n",
    "    beta_2=0.98,         # Adam 옵티마이저의 하이퍼파라미터 beta_2 설정\n",
    "    epsilon=1e-9         # Adam 옵티마이저의 하이퍼파라미터 epsilon 설정\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3c50f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparseCategoricalCrossentropy 손실 함수 객체 생성\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,     # 모델 출력이 로짓인 경우 True, 확률 분포인 경우 False\n",
    "    reduction='none'      # 손실을 각 샘플에 대해 계산하고, 각 샘플의 손실을 모두 더하지 않음\n",
    ")\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    # 패딩된 부분을 제외하고 유효한 부분만 마스킹\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "\n",
    "    # 모델 예측과 실제 값 간의 손실 계산\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    # 패딩 부분에 대한 마스킹 적용\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    # 마스킹된 부분의 손실을 총합하고, 마스킹의 총합으로 나눠 정규화\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e677d40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    '''\n",
    "    src: 인코더 입력\n",
    "    tgt: 디코더 입력\n",
    "    model: 트랜스포머 모델\n",
    "    optimizer: 사용할 최적화 알고리즘\n",
    "    '''\n",
    "    # Decoder의 입력과 최종 타겟 생성\n",
    "    tgt_in = tgt[:, :-1]  # Decoder의 input\n",
    "    gold = tgt[:, 1:]     # Decoder의 output과 비교하기 위해 right shift를 통해 생성한 최종 타겟\n",
    "\n",
    "    # 마스크 생성\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    # 그래디언트 계산을 위한 GradientTape 사용\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 모델 순전파 수행\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "\n",
    "        # 손실 함수 계산\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    # 그래디언트 계산 및 모델 파라미터 업데이트\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    # 손실과 어텐션 가중치 반환\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "496a1c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "    \n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        model(_input, \n",
    "              output,\n",
    "              enc_padding_mask,\n",
    "              combined_mask,\n",
    "              dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = tgt_tokenizer.decode_ids(ids)\n",
    "\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4567d708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_answer(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n",
    "    \n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72ec6cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train)).batch(batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc5e369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b263bb0de21f42fab524dd99a8f01a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "example = [\"지루하다, 놀러가고 싶어.\", \"오늘 일찍 일어났더니 피곤하다.\",\n",
    "                 \"간만에 여자친구랑 데이트 하기로 했어.\", \"집에 있는다는 소리야.\"]\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    # tqdm을 사용하여 진행 상황을 시각적으로 확인\n",
    "    dataset_count = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "    tqdm_bar = tqdm(total=dataset_count, desc=f'Epoch {epoch + 1}')\n",
    "\n",
    "    # 훈련 데이터셋의 각 배치에 대해 훈련 수행\n",
    "    for (batch, (src, tgt)) in enumerate(train_dataset):\n",
    "        # 훈련 스텝 수행\n",
    "        loss, _, _, _ = train_step(enc_train, dec_train, transformer, optimizer)\n",
    "        total_loss += loss\n",
    "\n",
    "        # tqdm 업데이트\n",
    "        tqdm_bar.update(1)\n",
    "        tqdm_bar.set_postfix({'loss': total_loss.numpy() / (batch + 1)})\n",
    "    \n",
    "    for example in examples:\n",
    "        translate_answer(example, transformer, tokenizer, tokenitokenizerzertokenizer)\n",
    "\n",
    "    # epoch 종료 후 평균 손실 출력\n",
    "    avg_loss = total_loss / dataset_count\n",
    "    print(f'Epoch {epoch + 1}, Loss: {avg_loss.numpy()}')\n",
    "    \n",
    "    tqdm_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c2bb7e",
   "metadata": {},
   "source": [
    "# 성능 측정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ad7d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    return sentence_bleu([reference],\n",
    "                         candidate,\n",
    "                         weights=weights,\n",
    "                         smoothing_function=SmoothingFunction().method1)  # smoothing_function 적용\n",
    "\n",
    "print(\"BLEU-1:\", calculate_bleu(reference, candidate, weights=[1, 0, 0, 0]))\n",
    "print(\"BLEU-2:\", calculate_bleu(reference, candidate, weights=[0, 1, 0, 0]))\n",
    "print(\"BLEU-3:\", calculate_bleu(reference, candidate, weights=[0, 0, 1, 0]))\n",
    "print(\"BLEU-4:\", calculate_bleu(reference, candidate, weights=[0, 0, 0, 1]))\n",
    "\n",
    "print(\"\\nBLEU-Total:\", calculate_bleu(reference, candidate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6227fb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(tokens, model, src_tokenizer, tgt_tokenizer):\n",
    "    # 입력 토큰을 패딩하여 모델에 전달\n",
    "    padded_tokens = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=MAX_LEN,\n",
    "                                                           padding='post')\n",
    "\n",
    "    # 디코더 입력 초기화\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
    "\n",
    "    # 디코딩된 토큰을 저장할 리스트 초기화\n",
    "    ids = []\n",
    "\n",
    "    # 최대 길이(MAX_LEN)만큼 반복하여 번역 수행\n",
    "    for i in range(MAX_LEN):\n",
    "        # 마스크 생성\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(padded_tokens, output)\n",
    "\n",
    "        # 모델에 입력하여 다음 토큰 예측\n",
    "        predictions, _, _, _ = model(padded_tokens, \n",
    "                                      output,\n",
    "                                      enc_padding_mask,\n",
    "                                      combined_mask,\n",
    "                                      dec_padding_mask)\n",
    "\n",
    "        # 예측된 확률 분포에서 가장 높은 확률을 가진 토큰 선택\n",
    "        predicted_id = tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        # 종료 토큰이 예측되면 번역 결과 반환\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)  \n",
    "            return result\n",
    "\n",
    "        # 예측된 토큰을 결과에 추가\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    # 최대 길이까지 종료 토큰이 나오지 않으면 현재까지 예측된 토큰 반환\n",
    "    result = tgt_tokenizer.decode_ids(ids)  \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a99e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bleu_single(model, src_sentence, tgt_sentence, src_tokenizer, tgt_tokenizer, verbose=True):\n",
    "    # 소스 문장 및 타겟 문장을 토큰화하여 인코딩\n",
    "    src_tokens = src_tokenizer.encode_as_ids(src_sentence)\n",
    "    tgt_tokens = tgt_tokenizer.encode_as_ids(tgt_sentence)\n",
    "\n",
    "    # 최대 길이를 초과하는 경우 평가하지 않음\n",
    "    if (len(src_tokens) > MAX_LEN) or (len(tgt_tokens) > MAX_LEN):\n",
    "        return None\n",
    "\n",
    "    # 타겟 문장을 기준으로 BLEU 점수 계산\n",
    "    reference = tgt_sentence.split()\n",
    "    candidate = translate(src_tokens, model, src_tokenizer, tgt_tokenizer).split()\n",
    "\n",
    "    # BLEU 점수 계산 및 출력\n",
    "    score = sentence_bleu([reference], candidate,\n",
    "                          smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Source Sentence: \", src_sentence)\n",
    "        print(\"Model Prediction: \", candidate)\n",
    "        print(\"Real: \", reference)\n",
    "        print(\"Score: %lf\\n\" % score)\n",
    "        \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17e9461",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = 0  # 테스트 인덱스\n",
    "\n",
    "# 테스트 인덱스의 영어 문장과 스페인어 실제 번역 문장을 사용하여 BLEU 점수 평가\n",
    "eval_bleu_single(transformer, \n",
    "                 enc_train[test_idx], \n",
    "                 dec_train[test_idx], \n",
    "                 tokenizer, \n",
    "                 tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586e3e2d",
   "metadata": {},
   "source": [
    "# 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad262428",
   "metadata": {},
   "source": [
    "- 여전히 갈 길은 멀고, 알다가도 모르겠는 알쏭달쏭함이 있다.\n",
    "- 시간을 들여서 익숙해질 필요가 있는 것 같다\n",
    "- 이론은 알겠는데, 코드로 구현하는 게 쉽지가 않은 것 같다.\n",
    "- 커널은 왜 자꾸 죽을까... \n",
    "- 학습과정에서 커널이 죽어서 아쉽지만 여기까지 온 것만으로도 감사하다. \n",
    "- 다음 프로젝트때는 더 나은 결과가 있기를! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
