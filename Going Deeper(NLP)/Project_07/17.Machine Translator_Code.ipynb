{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dada577",
   "metadata": {},
   "source": [
    "# 번역 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32683a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "import re\n",
    "import os \n",
    "import random\n",
    "import math\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e7ca88",
   "metadata": {},
   "source": [
    "영어-스페인어 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c86e9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip',\n",
    "    origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6475ab6",
   "metadata": {},
   "source": [
    "중복된 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1191d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: 118964\n",
      ">> I hate silent movies.\tMe carga ver cine mudo.\n",
      ">> You forgot to tell me about that book you read.\tSe te olvidó contarme sobre el libro que leíste.\n",
      ">> You must finish this work in a week.\tDebes terminar este trabajo en una semana.\n",
      ">> Go and fetch Tom.\tVe a traer a Tom.\n",
      ">> We gladly accept your offer.\tAceptamos con gusto tu oferta.\n"
     ]
    }
   ],
   "source": [
    "file_path = os.path.dirname(zip_path)+\"/spa-eng/spa.txt\"\n",
    "\n",
    "with open(file_path, \"r\") as f:\n",
    "    spa_eng_sentences = f.read().splitlines()\n",
    "\n",
    "spa_eng_sentences = list(set(spa_eng_sentences)) \n",
    "total_sentence_count = len(spa_eng_sentences)\n",
    "print(\"Example:\", total_sentence_count)\n",
    "\n",
    "for sen in spa_eng_sentences[0:100][::20]: \n",
    "    print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52a53ad",
   "metadata": {},
   "source": [
    "전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "548937a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower()# 소문자 변환\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 연속된 여러 개의 공백을 하나의 공백으로 축소\n",
    "    sentence = sentence.strip() # 문자열 양 끝 공백 제거\n",
    "    \n",
    "    return sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87cb57b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spa_eng_sentences = list(map(preprocess_sentence, spa_eng_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84c36a1",
   "metadata": {},
   "source": [
    "전체 데이터의 0.5% 정도를 테스트용으로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abecb555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Size:  594\n",
      "\n",
      "\n",
      "Train Example: 118370\n",
      ">> i hate silent movies.\tme carga ver cine mudo.\n",
      ">> you forgot to tell me about that book you read.\tse te olvidó contarme sobre el libro que leíste.\n",
      ">> you must finish this work in a week.\tdebes terminar este trabajo en una semana.\n",
      ">> go and fetch tom.\tve a traer a tom.\n",
      ">> we gladly accept your offer.\taceptamos con gusto tu oferta.\n",
      "\n",
      "\n",
      "Test Example: 594\n",
      ">> tom doesn't know.\ttom no lo sabe.\n",
      ">> i'll take care of the horses.\tyo cuidaré a los caballos.\n",
      ">> tom doesn't know what to expect.\ttom no sabe qué esperar.\n",
      ">> mary really takes after her mother.\tmary se parece mucho a su madre.\n",
      ">> tom doesn't know how to drive.\ttom no sabe manejar.\n"
     ]
    }
   ],
   "source": [
    "test_sentence_count = total_sentence_count // 200\n",
    "print(\"Test Size: \", test_sentence_count)\n",
    "print(\"\\n\")\n",
    "\n",
    "train_spa_eng_sentences = spa_eng_sentences[:-test_sentence_count]\n",
    "test_spa_eng_sentences = spa_eng_sentences[-test_sentence_count:]\n",
    "print(\"Train Example:\", len(train_spa_eng_sentences))\n",
    "for sen in train_spa_eng_sentences[0:100][::20]: \n",
    "    print(\">>\", sen)\n",
    "print(\"\\n\")\n",
    "print(\"Test Example:\", len(test_spa_eng_sentences))\n",
    "for sen in test_spa_eng_sentences[0:100][::20]: \n",
    "    print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f8c6f9",
   "metadata": {},
   "source": [
    "한 줄에 포함되어 있는 영어와 스페인어를 분리 - > split('\\t')를 사용   \n",
    "tab 이전이 영어, 이후가 스페인어 문장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e08da69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_spa_eng_sentences(spa_eng_sentences):\n",
    "    spa_sentences = []\n",
    "    eng_sentences = []\n",
    "    for spa_eng_sentence in tqdm(spa_eng_sentences):\n",
    "        eng_sentence, spa_sentence = spa_eng_sentence.split('\\t')\n",
    "        spa_sentences.append(spa_sentence)\n",
    "        eng_sentences.append(eng_sentence)\n",
    "    return eng_sentences, spa_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10b43568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_spa_eng_sentences(spa_eng_sentences):\n",
    "    spa_sentences = []\n",
    "    eng_sentences = []\n",
    "    for spa_eng_sentence in tqdm(spa_eng_sentences):\n",
    "        try:\n",
    "            eng_sentence, spa_sentence = spa_eng_sentence.split('\\t')\n",
    "            spa_sentences.append(spa_sentence)\n",
    "            eng_sentences.append(eng_sentence)\n",
    "        except ValueError:\n",
    "            print(\"Error in sentence:\", spa_eng_sentence)\n",
    "\n",
    "    return eng_sentences, spa_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc071eb",
   "metadata": {},
   "source": [
    "학습 데이터와 테스트 데이터 모두 영어와 스페인어 문장 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "410fe78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bbe2d327bc5415eb7a377a08052d827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118370 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118370\n",
      "i hate silent movies.\n",
      "\n",
      "\n",
      "118370\n",
      "me carga ver cine mudo.\n"
     ]
    }
   ],
   "source": [
    "train_eng_sentences, train_spa_sentences = split_spa_eng_sentences(train_spa_eng_sentences)\n",
    "print(len(train_eng_sentences))\n",
    "print(train_eng_sentences[0])\n",
    "print('\\n')\n",
    "print(len(train_spa_sentences))\n",
    "print(train_spa_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd6f7ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a05fb0270e4c4baaadcd7adc7f8c7f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594\n",
      "tom doesn't know.\n",
      "\n",
      "\n",
      "594\n",
      "tom no lo sabe.\n"
     ]
    }
   ],
   "source": [
    "test_eng_sentences, test_spa_sentences = split_spa_eng_sentences(test_spa_eng_sentences)\n",
    "print(len(test_eng_sentences))\n",
    "print(test_eng_sentences[0])\n",
    "print('\\n')\n",
    "print(len(test_spa_sentences))\n",
    "print(test_spa_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2877f0",
   "metadata": {},
   "source": [
    "토큰화 -> Sentencepiece 기반의 토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c2a20ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tokenizer(corpus,\n",
    "                       vocab_size,\n",
    "                       lang=\"spa-eng\",\n",
    "                       pad_id=0,   # pad token의 일련번호\n",
    "                       bos_id=1,  # 문장의 시작을 의미하는 bos token(<s>)의 일련번호\n",
    "                       eos_id=2,  # 문장의 끝을 의미하는 eos token(</s>)의 일련번호\n",
    "                       unk_id=3):   # unk token의 일련번호\n",
    "    file = \"./%s_corpus.txt\" % lang\n",
    "    model = \"%s_spm\" % lang\n",
    "\n",
    "    with open(file, 'w') as f:\n",
    "        for row in corpus: f.write(str(row) + '\\n')\n",
    "\n",
    "    import sentencepiece as spm\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        '--input=./%s --model_prefix=%s --vocab_size=%d'\\\n",
    "        % (file, model, vocab_size) + \\\n",
    "        '--pad_id==%d --bos_id=%d --eos_id=%d --unk_id=%d'\\\n",
    "        % (pad_id, bos_id, eos_id, unk_id)\n",
    "    )\n",
    "\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.Load('%s.model' % model)\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7371fed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=././spa-eng_corpus.txt --model_prefix=spa-eng_spm --vocab_size=20000--pad_id==0 --bos_id=1 --eos_id=2 --unk_id=3\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ././spa-eng_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: spa-eng_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 20000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  e"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "os_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: ././spa-eng_corpus.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 236740 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=7876619\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9546% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=43\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999546\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 236740 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 110186 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 236740\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 62405\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 62405 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=35496 obj=10.753 num_tokens=118479 num_tokens/piece=3.33781\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=28146 obj=8.52387 num_tokens=118966 num_tokens/piece=4.22675\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=21998 obj=8.47819 num_tokens=125691 num_tokens/piece=5.71375\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=21981 obj=8.46424 num_tokens=125748 num_tokens/piece=5.72076\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: spa-eng_spm.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: spa-eng_spm.vocab\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 20000 # 단어사전수\n",
    "tokenizer = generate_tokenizer(train_eng_sentences + train_spa_sentences, VOCAB_SIZE, 'spa-eng')\n",
    "tokenizer.set_encode_extra_options(\"bos:eos\")  # 문장 양 끝에 <s> , </s> 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21a22113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토콘화 해주는 함수 저의\n",
    "def make_corpus(sentences, tokenizer):\n",
    "    corpus = []\n",
    "    for sentence in tqdm(sentences):\n",
    "        tokens = tokenizer.encode_as_ids(sentence)\n",
    "        corpus.append(tokens)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f558ab7",
   "metadata": {},
   "source": [
    "영어와 스페인어를 각각 토큰화    \n",
    "훈련 데이터만 토큰화    \n",
    "같은 토크나이저를 사용함   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46d0deb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0efa5a1d168f490f90c33e44803cb13c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118370 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76358ad3689d44b8bd7955714a3de074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118370 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eng_corpus = make_corpus(train_eng_sentences, tokenizer)\n",
    "spa_corpus = make_corpus(train_spa_sentences, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35fbbbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i hate silent movies.\n",
      "[1, 6, 670, 3351, 1534, 0, 2]\n",
      "\n",
      "\n",
      "me carga ver cine mudo.\n",
      "[1, 20, 4732, 261, 1731, 10081, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "print(train_eng_sentences[0])\n",
    "print(eng_corpus[0])\n",
    "print('\\n')\n",
    "print(train_spa_sentences[0])\n",
    "print(spa_corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc0be3c",
   "metadata": {},
   "source": [
    "pad_sequences()를 이용해서 한문장의 토큰 길이가 50이 되도록 해서 데이터셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3d139e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 50\n",
    "enc_ndarray = tf.keras.preprocessing.sequence.pad_sequences(eng_corpus, maxlen=MAX_LEN, padding='post')\n",
    "dec_ndarray = tf.keras.preprocessing.sequence.pad_sequences(spa_corpus, maxlen=MAX_LEN, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4156e3",
   "metadata": {},
   "source": [
    "모델 훈련에 사용될 수 있도록 영어와 스페인어 데이터를 묶어 배치 크기의 텐서로 만들어 줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f0b55d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((enc_ndarray, dec_ndarray)).batch(batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf79fbf",
   "metadata": {},
   "source": [
    "# 트랜스포머 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa36074",
   "metadata": {},
   "source": [
    "Positional Encoding: 모델에 입력되는 단어의 상대적인 위치 정보를 반영하여 모델이 문장의 순서를 학습할 수 있도록 도움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15b9d782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    # 각 포지션과 차원에 대한 각도를 계산하는 함수\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, (2*(i//2)) / np.float32(d_model))\n",
    "\n",
    "    # 각 포지션에 대한 각도 벡터를 생성하는 함수\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    # 각 포지션과 차원에 대한 사인과 코사인 값을 가지는 테이블을 생성\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    # 짝수 차원에는 사인 값을, 홀수 차원에는 코사인 값을 적용\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # 짝수 차원\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # 홀수 차원\n",
    "\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0e3df6",
   "metadata": {},
   "source": [
    "마스크 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcc9dd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    # 입력 시퀀스에서 패딩(0)에 해당하는 위치를 찾아 마스크 생성\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_lookahead_mask(size):\n",
    "    # 룩어헤드 마스크 생성: 현재 위치 이후의 위치에 대해 마스킹\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    # 인코더 입력 패딩 마스크 생성\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "\n",
    "    # 디코더-인코더 어텐션 마스크 생성 (인코더 입력의 패딩 위치 마스킹)\n",
    "    dec_enc_mask = generate_padding_mask(src)\n",
    "\n",
    "    # 디코더 마스크 생성 (패딩 위치와 룩어헤드 마스킹을 포함)\n",
    "    dec_lookahead_mask = generate_lookahead_mask(tgt.shape[1])\n",
    "    dec_tgt_padding_mask = generate_padding_mask(tgt)\n",
    "    dec_mask = tf.maximum(dec_tgt_padding_mask, dec_lookahead_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b671a3",
   "metadata": {},
   "source": [
    "Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77ccefef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        # 각각의 선형 변환을 수행하는 레이어들을 정의\n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        # 마지막에 사용되는 선형 변환 레이어\n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        # 스케일링된 점곱 어텐션을 수행하는 메소드\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        # 마스크 적용 (필요한 경우)\n",
    "        if mask is not None:\n",
    "            scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        # 소프트맥스 함수를 통해 어텐션 가중치 계산\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        # 가중 평균을 통해 출력 계산\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        # 주어진 텐서를 여러 헤드로 나누는 메소드\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        # 나누어진 헤드를 결합하는 메소드\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "    \n",
    "    def call(self, Q, K, V, mask):\n",
    "        # 주어진 Q, K, V에 대한 멀티헤드 어텐션을 수행하는 메소드\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        # 각각의 헤드로 나누기\n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "        \n",
    "        # 스케일드 닷 프로덕트 어텐션 수행\n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "                        \n",
    "        # 헤드 결합\n",
    "        out = self.combine_heads(out)\n",
    "        # 최종 선형 변환 수행\n",
    "        out = self.linear(out)\n",
    "            \n",
    "        return out, attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0dd2ec",
   "metadata": {},
   "source": [
    "Position-wise Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4845d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        # 첫 번째 fully-connected layer (linear layer) - 활성화 함수는 ReLU\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        # 두 번째 fully-connected layer (linear layer)\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        # 주어진 입력을 첫 번째 fully-connected layer에 전달하고 ReLU 활성화 함수를 적용\n",
    "        out = self.fc1(x)\n",
    "        # 결과를 두 번째 fully-connected layer에 전달\n",
    "        out = self.fc2(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f89398",
   "metadata": {},
   "source": [
    "Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "563b28ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        # 멀티헤드 어텐션 레이어\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        # 위치별 피드포워드 네트워크 레이어\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        # 각 레이어마다의 레이어 정규화 레이어\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        # 드롭아웃 레이어\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        # Residual Connection을 위한 원본 입력 저장\n",
    "        residual = x\n",
    "        # 첫 번째 레이어 정규화\n",
    "        out = self.norm_1(x)\n",
    "        # 멀티헤드 어텐션 레이어 수행\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        # 드롭아웃 적용\n",
    "        out = self.do(out)\n",
    "        # Residual Connection 수행\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        # Residual Connection을 위한 원본 입력 저장\n",
    "        residual = out\n",
    "        # 두 번째 레이어 정규화\n",
    "        out = self.norm_2(out)\n",
    "        # 위치별 피드포워드 네트워크 레이어 수행\n",
    "        out = self.ffn(out)\n",
    "        # 드롭아웃 적용\n",
    "        out = self.do(out)\n",
    "        # Residual Connection 수행\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d239232a",
   "metadata": {},
   "source": [
    "Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd1e8063",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        # 디코더 자체 어텐션 레이어\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        # 디코더-인코더 어텐션 레이어\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        # 위치별 피드포워드 네트워크 레이어\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        # 각 레이어마다의 레이어 정규화 레이어\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        # 드롭아웃 레이어\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        '''\n",
    "        Masked Multi-Head Attention\n",
    "        '''\n",
    "        # Residual Connection을 위한 원본 입력 저장\n",
    "        residual = x\n",
    "        # 첫 번째 레이어 정규화\n",
    "        out = self.norm_1(x)\n",
    "        # 디코더 자체 어텐션 레이어 수행\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        # 드롭아웃 적용\n",
    "        out = self.do(out)\n",
    "        # Residual Connection 수행\n",
    "        out += residual\n",
    "\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        # Residual Connection을 위한 원본 입력 저장\n",
    "        residual = out\n",
    "        # 두 번째 레이어 정규화\n",
    "        out = self.norm_2(out)\n",
    "        # 디코더-인코더 어텐션 레이어 수행\n",
    "        # Q, K, V 순서에 주의하세요!\n",
    "        out, dec_enc_attn = self.enc_dec_attn(Q=out, K=enc_out, V=enc_out, mask=dec_enc_mask)\n",
    "        # 드롭아웃 적용\n",
    "        out = self.do(out)\n",
    "        # Residual Connection 수행\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        # Residual Connection을 위한 원본 입력 저장\n",
    "        residual = out\n",
    "        # 세 번째 레이어 정규화\n",
    "        out = self.norm_3(out)\n",
    "        # 위치별 피드포워드 네트워크 레이어 수행\n",
    "        out = self.ffn(out)\n",
    "        # 드롭아웃 적용\n",
    "        out = self.do(out)\n",
    "        # Residual Connection 수행\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2f78d9",
   "metadata": {},
   "source": [
    "Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "070817da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # 인코더 레이어를 n_layers 개수만큼 생성\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                           for _ in range(n_layers)]\n",
    "        \n",
    "        # 드롭아웃 레이어\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        # 입력 데이터 초기화\n",
    "        out = x\n",
    "    \n",
    "        # 각 인코더 레이어를 순회하면서 호출\n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            # 인코더 레이어 호출\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            # 어텐션 가중치 저장\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6cb70b",
   "metadata": {},
   "source": [
    "Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8c1712b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # 디코더 레이어를 n_layers 개수만큼 생성\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                           for _ in range(n_layers)]\n",
    "                            \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        # 입력 데이터 초기화\n",
    "        out = x\n",
    "    \n",
    "        # 각 디코더 레이어를 순회하면서 호출\n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            # 디코더 레이어 호출\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "                self.dec_layers[i](out, enc_out, dec_enc_mask, padding_mask)\n",
    "\n",
    "            # 각 레이어에서 얻은 어텐션 가중치 저장\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        # 최종 디코더 레이어의 출력과 각 레이어에서의 어텐션 가중치 리스트 반환\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a40260",
   "metadata": {},
   "source": [
    "Transformer 전체 모델 조립"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb4a9025",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, src_vocab_size, tgt_vocab_size, pos_len, dropout=0.2, shared_fc=True, shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # 입력으로 받은 d_model을 float32 타입으로 변경\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        # 공유 임베딩 여부에 따라 인코더와 디코더의 임베딩 레이어 생성\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        # 위치 인코딩 레이어 생성\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        # 인코더와 디코더 생성\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        # 출력 레이어 생성\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        # 공유 출력 레이어 여부에 따라 가중치 공유\n",
    "        self.shared_fc = shared_fc\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        # 입력에 임베딩 레이어 적용\n",
    "        out = emb(x)\n",
    "\n",
    "        # 공유된 출력 레이어를 사용할 경우 sqrt(d_model)로 스케일 조정\n",
    "        if self.shared_fc:\n",
    "            out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        # 위치 인코딩 추가\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def call(self, enc_in, dec_in, enc_mask, dec_enc_mask, dec_mask):\n",
    "        # 인코더 입력과 디코더 입력에 임베딩 적용\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        # 인코더와 디코더 순전파\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        dec_out, dec_attns, dec_enc_attns = self.decoder(dec_in, enc_out, dec_enc_mask, dec_mask)\n",
    "\n",
    "        # 출력 레이어를 통과한 최종 로짓 반환\n",
    "        logits = self.fc(dec_out)\n",
    "\n",
    "        # 로짓, 인코더 어텐션, 디코더 어텐션, 디코더-인코더 어텐션 반환\n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c80b6b",
   "metadata": {},
   "source": [
    "모델 인스턴스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "293e47fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\n",
    "d_model = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f5227b",
   "metadata": {},
   "source": [
    "Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e96ae84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        \n",
    "        # 모델의 임베딩 차원과 워머핑 스텝 수 초기화\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        # 학습률을 조절하기 위한 함수 정의\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        # 두 가지 학습률 조절 방법 중 작은 값 선택\n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af6aa14",
   "metadata": {},
   "source": [
    "Learning Rate & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6617d46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LearningRateScheduler를 사용하여 학습률 스케줄링 설정\n",
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "# Adam 옵티마이저 설정\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate,      # 학습률은 LearningRateScheduler에 의해 동적으로 결정됨\n",
    "    beta_1=0.9,          # Adam 옵티마이저의 하이퍼파라미터 beta_1 설정\n",
    "    beta_2=0.98,         # Adam 옵티마이저의 하이퍼파라미터 beta_2 설정\n",
    "    epsilon=1e-9         # Adam 옵티마이저의 하이퍼파라미터 epsilon 설정\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f6540c",
   "metadata": {},
   "source": [
    "Loss Function 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3715e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparseCategoricalCrossentropy 손실 함수 객체 생성\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,     # 모델 출력이 로짓인 경우 True, 확률 분포인 경우 False\n",
    "    reduction='none'      # 손실을 각 샘플에 대해 계산하고, 각 샘플의 손실을 모두 더하지 않음\n",
    ")\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    # 패딩된 부분을 제외하고 유효한 부분만 마스킹\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "\n",
    "    # 모델 예측과 실제 값 간의 손실 계산\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    # 패딩 부분에 대한 마스킹 적용\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    # 마스킹된 부분의 손실을 총합하고, 마스킹의 총합으로 나눠 정규화\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6ff5e5",
   "metadata": {},
   "source": [
    "Train Step 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ff3b664",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    '''\n",
    "    src: 인코더 입력\n",
    "    tgt: 디코더 입력\n",
    "    model: 트랜스포머 모델\n",
    "    optimizer: 사용할 최적화 알고리즘\n",
    "    '''\n",
    "    # Decoder의 입력과 최종 타겟 생성\n",
    "    tgt_in = tgt[:, :-1]  # Decoder의 input\n",
    "    gold = tgt[:, 1:]     # Decoder의 output과 비교하기 위해 right shift를 통해 생성한 최종 타겟\n",
    "\n",
    "    # 마스크 생성\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    # 그래디언트 계산을 위한 GradientTape 사용\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 모델 순전파 수행\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "\n",
    "        # 손실 함수 계산\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    # 그래디언트 계산 및 모델 파라미터 업데이트\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    # 손실과 어텐션 가중치 반환\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf27f2a",
   "metadata": {},
   "source": [
    "훈련을 시키자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3542c14e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "716a9f6a62594ecf8a1ccba95c841ecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/1850 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 3.7447965145111084\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d4ddf9c0924416b8824238255c95306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/1850 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 2.1256184577941895\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e2ee136c534fb48dcafeaaef547a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/1850 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 1.792917251586914\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    # tqdm을 사용하여 진행 상황을 시각적으로 확인\n",
    "    dataset_count = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "    tqdm_bar = tqdm(total=dataset_count, desc=f'Epoch {epoch + 1}')\n",
    "\n",
    "    # 훈련 데이터셋의 각 배치에 대해 훈련 수행\n",
    "    for (batch, (src, tgt)) in enumerate(train_dataset):\n",
    "        # 훈련 스텝 수행\n",
    "        loss, _, _, _ = train_step(src, tgt, transformer, optimizer)\n",
    "        total_loss += loss\n",
    "\n",
    "        # tqdm 업데이트\n",
    "        tqdm_bar.update(1)\n",
    "        tqdm_bar.set_postfix({'loss': total_loss.numpy() / (batch + 1)})\n",
    "\n",
    "    # epoch 종료 후 평균 손실 출력\n",
    "    avg_loss = total_loss / dataset_count\n",
    "    print(f'Epoch {epoch + 1}, Loss: {avg_loss.numpy()}')\n",
    "    tqdm_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b56aa7",
   "metadata": {},
   "source": [
    "# 번역 성능 측정하기\n",
    "## BLEU Score\n",
    "-  N-gram으로 점수를 측정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32baacc",
   "metadata": {},
   "source": [
    "NLTK를 활용한 BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9fdafe1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문: ['많', '은', '자연어', '처리', '연구자', '들', '이', '트랜스포머', '를', '선호', '한다']\n",
      "번역문: ['적', '은', '자연어', '학', '개발자', '들', '가', '트랜스포머', '을', '선호', '한다', '요']\n",
      "BLEU Score: 8.190757052088229e-155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.9/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# 아래 두 문장을 바꿔가며 테스트 해보세요\n",
    "reference = \"많 은 자연어 처리 연구자 들 이 트랜스포머 를 선호 한다\".split()\n",
    "candidate = \"적 은 자연어 학 개발자 들 가 트랜스포머 을 선호 한다 요\".split()\n",
    "\n",
    "print(\"원문:\", reference)\n",
    "print(\"번역문:\", candidate)\n",
    "print(\"BLEU Score:\", sentence_bleu([reference], candidate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10178f2b",
   "metadata": {},
   "source": [
    "BLEU Score는 0~1 사이의 값을 가지지만, 100을 곱한 백분율 값으로 표기하는 경우도 많다   \n",
    "BLEU Score가 50점을 넘는다는 것은 정말 멋진 번역을 생성했다는 의미이지만, N-gram별로 확인해 볼 필요가 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "51ca31b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram: 0.5\n",
      "2-gram: 0.18181818181818182\n",
      "3-gram: 2.2250738585072626e-308\n",
      "4-gram: 2.2250738585072626e-308\n"
     ]
    }
   ],
   "source": [
    "print(\"1-gram:\", sentence_bleu([reference], candidate, weights=[1, 0, 0, 0]))\n",
    "print(\"2-gram:\", sentence_bleu([reference], candidate, weights=[0, 1, 0, 0]))\n",
    "print(\"3-gram:\", sentence_bleu([reference], candidate, weights=[0, 0, 1, 0]))\n",
    "print(\"4-gram:\", sentence_bleu([reference], candidate, weights=[0, 0, 0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2670925f",
   "metadata": {},
   "source": [
    "BLEU 계산시 특정 N-gram이 0점이 나와서 BLEU가 너무 커지거나 작아지는 쪽으로 왜곡되는 문제를 보완하기 위해 SmoothingFunction() 을 사용하고 있다  \n",
    "Smoothing 함수는 모든 Precision에 아주 작은 epsilon 값을 더해주는 역할을 하는데, 이로써 0점이 부여된 Precision도 완전한 0이 되지 않으니 점수를 1.0 으로 대체할 필요가 없어짐  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "16c7583d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.5\n",
      "BLEU-2: 0.18181818181818182\n",
      "BLEU-3: 0.010000000000000004\n",
      "BLEU-4: 0.011111111111111112\n",
      "\n",
      "BLEU-Total: 0.05637560315259291\n"
     ]
    }
   ],
   "source": [
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    return sentence_bleu([reference],\n",
    "                         candidate,\n",
    "                         weights=weights,\n",
    "                         smoothing_function=SmoothingFunction().method1)  # smoothing_function 적용\n",
    "\n",
    "print(\"BLEU-1:\", calculate_bleu(reference, candidate, weights=[1, 0, 0, 0]))\n",
    "print(\"BLEU-2:\", calculate_bleu(reference, candidate, weights=[0, 1, 0, 0]))\n",
    "print(\"BLEU-3:\", calculate_bleu(reference, candidate, weights=[0, 0, 1, 0]))\n",
    "print(\"BLEU-4:\", calculate_bleu(reference, candidate, weights=[0, 0, 0, 1]))\n",
    "\n",
    "print(\"\\nBLEU-Total:\", calculate_bleu(reference, candidate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b84885",
   "metadata": {},
   "source": [
    "결과: 거의 의미 없는 번역"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e714d8",
   "metadata": {},
   "source": [
    "## 트랜스포머 모델의 번역 성능 알아보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0898b6",
   "metadata": {},
   "source": [
    "테스트셋으로 모델의 BLEU Score를 측정하는 함수 eval_bleu() 를 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "21118a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(tokens, model, src_tokenizer, tgt_tokenizer):\n",
    "    # 입력 토큰을 패딩하여 모델에 전달\n",
    "    padded_tokens = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=MAX_LEN,\n",
    "                                                           padding='post')\n",
    "\n",
    "    # 디코더 입력 초기화\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
    "\n",
    "    # 디코딩된 토큰을 저장할 리스트 초기화\n",
    "    ids = []\n",
    "\n",
    "    # 최대 길이(MAX_LEN)만큼 반복하여 번역 수행\n",
    "    for i in range(MAX_LEN):\n",
    "        # 마스크 생성\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(padded_tokens, output)\n",
    "\n",
    "        # 모델에 입력하여 다음 토큰 예측\n",
    "        predictions, _, _, _ = model(padded_tokens, \n",
    "                                      output,\n",
    "                                      enc_padding_mask,\n",
    "                                      combined_mask,\n",
    "                                      dec_padding_mask)\n",
    "\n",
    "        # 예측된 확률 분포에서 가장 높은 확률을 가진 토큰 선택\n",
    "        predicted_id = tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        # 종료 토큰이 예측되면 번역 결과 반환\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)  \n",
    "            return result\n",
    "\n",
    "        # 예측된 토큰을 결과에 추가\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    # 최대 길이까지 종료 토큰이 나오지 않으면 현재까지 예측된 토큰 반환\n",
    "    result = tgt_tokenizer.decode_ids(ids)  \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d17213",
   "metadata": {},
   "source": [
    "번역한 문장의 BLEU Score를 평가할 수 있도록 함수를 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8e308c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bleu_single(model, src_sentence, tgt_sentence, src_tokenizer, tgt_tokenizer, verbose=True):\n",
    "    # 소스 문장 및 타겟 문장을 토큰화하여 인코딩\n",
    "    src_tokens = src_tokenizer.encode_as_ids(src_sentence)\n",
    "    tgt_tokens = tgt_tokenizer.encode_as_ids(tgt_sentence)\n",
    "\n",
    "    # 최대 길이를 초과하는 경우 평가하지 않음\n",
    "    if (len(src_tokens) > MAX_LEN) or (len(tgt_tokens) > MAX_LEN):\n",
    "        return None\n",
    "\n",
    "    # 타겟 문장을 기준으로 BLEU 점수 계산\n",
    "    reference = tgt_sentence.split()\n",
    "    candidate = translate(src_tokens, model, src_tokenizer, tgt_tokenizer).split()\n",
    "\n",
    "    # BLEU 점수 계산 및 출력\n",
    "    score = sentence_bleu([reference], candidate,\n",
    "                          smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Source Sentence: \", src_sentence)\n",
    "        print(\"Model Prediction: \", candidate)\n",
    "        print(\"Real: \", reference)\n",
    "        print(\"Score: %lf\\n\" % score)\n",
    "        \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6f3446b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sentence:  tom doesn't know.\n",
      "Model Prediction:  ['tom', 'no', 'sabe', 'nada', 'de', 'él', 'a', 'mary?']\n",
      "Real:  ['tom', 'no', 'lo', 'sabe.']\n",
      "Score: 0.058739\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05873949094699214"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_idx = 0  # 테스트 인덱스\n",
    "\n",
    "# 테스트 인덱스의 영어 문장과 스페인어 실제 번역 문장을 사용하여 BLEU 점수 평가\n",
    "eval_bleu_single(transformer, \n",
    "                 test_eng_sentences[test_idx], \n",
    "                 test_spa_sentences[test_idx], \n",
    "                 tokenizer, \n",
    "                 tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287bcb30",
   "metadata": {},
   "source": [
    "전체 테스트 데이터에 대해서 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8d1835db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bleu(model, src_sentences, tgt_sentences, src_tokenizer, tgt_tokenizer, verbose=True):\n",
    "    total_score = 0.0\n",
    "    sample_size = len(src_sentences)\n",
    "    \n",
    "    # 각각의 테스트 샘플에 대해 BLEU 점수 계산\n",
    "    for idx in tqdm(range(sample_size)):\n",
    "        score = eval_bleu_single(model, src_sentences[idx], tgt_sentences[idx], src_tokenizer, tgt_tokenizer, verbose)\n",
    "        if not score: continue\n",
    "        \n",
    "        total_score += score\n",
    "    \n",
    "    # 전체 테스트 샘플에 대한 평균 BLEU 점수 출력\n",
    "    print(\"Num of Sample:\", sample_size)\n",
    "    print(\"Total Score:\", total_score / sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc90d7d",
   "metadata": {},
   "source": [
    "Transformer 모델을 사용하여 테스트 데이터셋의 모든 샘플에 대한 BLEU 점수를 평가하고, 평균 BLEU 점수를 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "166c2060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b03aa8fdee7b44789b39b2b11a3942ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Sample: 594\n",
      "Total Score: 0.09492075419921593\n"
     ]
    }
   ],
   "source": [
    "eval_bleu(transformer, test_eng_sentences, test_spa_sentences, tokenizer, tokenizer, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc267c92",
   "metadata": {},
   "source": [
    "## Beam Search Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "be9b2caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decoder(prob, beam_size):\n",
    "    sequences = [[[], 1.0]]  # 생성된 문장과 점수를 저장\n",
    "\n",
    "    for tok in prob:\n",
    "        all_candidates = []\n",
    "\n",
    "        for seq, score in sequences:\n",
    "            for idx, p in enumerate(tok): # 각 단어의 확률을 총점에 누적 곱\n",
    "                candidate = [seq + [idx], score * -math.log(-(p-1))]\n",
    "                all_candidates.append(candidate)\n",
    "\n",
    "        ordered = sorted(all_candidates,\n",
    "                         key=lambda tup:tup[1],\n",
    "                         reverse=True) # 총점 순 정렬\n",
    "        sequences = ordered[:beam_size] # Beam Size에 해당하는 문장만 저장 \n",
    "\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "25fd928a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "커피 를 가져 도 될 까요? <pad> <pad> <pad> <pad>  // Score: 42.5243\n",
      "커피 를 마셔 도 될 까요? <pad> <pad> <pad> <pad>  // Score: 28.0135\n",
      "마셔 를 가져 도 될 까요? <pad> <pad> <pad> <pad>  // Score: 17.8983\n"
     ]
    }
   ],
   "source": [
    "vocab = {\n",
    "    0: \"<pad>\",\n",
    "    1: \"까요?\",\n",
    "    2: \"커피\",\n",
    "    3: \"마셔\",\n",
    "    4: \"가져\",\n",
    "    5: \"될\",\n",
    "    6: \"를\",\n",
    "    7: \"한\",\n",
    "    8: \"잔\",\n",
    "    9: \"도\",\n",
    "}\n",
    "\n",
    "prob_seq = [[0.01, 0.01, 0.60, 0.32, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.75, 0.01, 0.01, 0.17],\n",
    "            [0.01, 0.01, 0.01, 0.35, 0.48, 0.10, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.24, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.68],\n",
    "            [0.01, 0.01, 0.12, 0.01, 0.01, 0.80, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.01, 0.81, 0.01, 0.01, 0.01, 0.01, 0.11, 0.01, 0.01, 0.01],\n",
    "            [0.70, 0.22, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]]\n",
    "\n",
    "prob_seq = np.array(prob_seq)\n",
    "beam_size = 3\n",
    "\n",
    "result = beam_search_decoder(prob_seq, beam_size)\n",
    "\n",
    "for seq, score in result:\n",
    "    sentence = \"\"\n",
    "\n",
    "    for word in seq:\n",
    "        sentence += vocab[word] + \" \"\n",
    "\n",
    "    print(sentence, \"// Score: %.4f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d4d57e",
   "metadata": {},
   "source": [
    "Beam Search를 생성 기법으로 구현할 때에는 분기를 잘 나눠줘야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ed2d5f",
   "metadata": {},
   "source": [
    "### Beam Search Decoder 작성 및 평가하기\n",
    "각 단어의 확률값을 계산하는 calc_prob()와 Beam Search를 기반으로 동작하는 beam_search_decoder() 를 구현하고 생성된 문장에 대해 BLEU Score를 출력하는 beam_bleu() 를 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7e13d00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_prob(src_ids, tgt_ids, model):\n",
    "    # 마스크 생성\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "    generate_masks(src_ids, tgt_ids)\n",
    "\n",
    "    # 모델에 입력하여 다음 토큰의 확률 분포를 예측\n",
    "    predictions, _, _, _ = model(src_ids, \n",
    "                                 tgt_ids,\n",
    "                                 enc_padding_mask,\n",
    "                                 combined_mask,\n",
    "                                 dec_padding_mask)\n",
    "    \n",
    "    # 소프트맥스 함수를 통해 확률로 변환하여 반환\n",
    "    return tf.math.softmax(predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f7b3a935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decoder(sentence, \n",
    "                        src_len,\n",
    "                        tgt_len,\n",
    "                        model,\n",
    "                        src_tokenizer,\n",
    "                        tgt_tokenizer,\n",
    "                        beam_size):\n",
    "    # 소스 문장을 토큰으로 변환\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "    \n",
    "    # 패딩 및 길이 조정\n",
    "    src_in = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                            maxlen=src_len,\n",
    "                                                            padding='post')\n",
    "\n",
    "    # 예측 캐시 및 임시 예측 배열 초기화\n",
    "    pred_cache = np.zeros((beam_size * beam_size, tgt_len), dtype=np.int64)\n",
    "    pred_tmp = np.zeros((beam_size, tgt_len), dtype=np.int64)\n",
    "\n",
    "    # EOS 플래그 및 스코어 초기화\n",
    "    eos_flag = np.zeros((beam_size, ), dtype=np.int64)\n",
    "    scores = np.ones((beam_size, ))\n",
    "\n",
    "    # 첫 번째 타임 스텝의 시작 토큰 설정\n",
    "    pred_tmp[:, 0] = tgt_tokenizer.bos_id()\n",
    "\n",
    "    # 첫 번째 타임 스텝의 확률 계산\n",
    "    dec_in = tf.expand_dims(pred_tmp[0, :1], 0)\n",
    "    prob = calc_prob(src_in, dec_in, model)[0, -1].numpy()\n",
    "\n",
    "    # 루프를 통한 각 타임 스텝에 대한 빔 서치\n",
    "    for seq_pos in range(1, tgt_len):\n",
    "        score_cache = np.ones((beam_size * beam_size, ))\n",
    "\n",
    "        # 이전 타임 스텝의 예측을 캐시에 저장\n",
    "        for branch_idx in range(beam_size):\n",
    "            cache_pos = branch_idx * beam_size\n",
    "\n",
    "            score_cache[cache_pos:cache_pos+beam_size] = scores[branch_idx]\n",
    "            pred_cache[cache_pos:cache_pos+beam_size, :seq_pos] = \\\n",
    "            pred_tmp[branch_idx, :seq_pos]\n",
    "\n",
    "        # 현재 타임 스텝에서의 각 Branch에 대한 예측 계산\n",
    "        for branch_idx in range(beam_size):\n",
    "            cache_pos = branch_idx * beam_size\n",
    "\n",
    "            # 모든 Branch를 로 시작하는 경우를 방지\n",
    "            if seq_pos != 1:\n",
    "                dec_in = pred_cache[branch_idx, :seq_pos]\n",
    "                dec_in = tf.expand_dims(dec_in, 0)\n",
    "\n",
    "                # 현재 타임 스텝에서의 확률 계산\n",
    "                prob = calc_prob(src_in, dec_in, model)[0, -1].numpy()\n",
    "\n",
    "            # 각 Beam에 대한 확률 계산 및 최적 예측 저장\n",
    "            for beam_idx in range(beam_size):\n",
    "                max_idx = np.argmax(prob)\n",
    "\n",
    "                score_cache[cache_pos+beam_idx] *= prob[max_idx]\n",
    "                pred_cache[cache_pos+beam_idx, seq_pos] = max_idx\n",
    "\n",
    "                prob[max_idx] = -1\n",
    "\n",
    "        # 각 Branch에서의 최종 예측 및 스코어 업데이트\n",
    "        for beam_idx in range(beam_size):\n",
    "            if eos_flag[beam_idx] == -1: continue\n",
    "\n",
    "            # 가장 높은 스코어를 갖는 예측 선택\n",
    "            max_idx = np.argmax(score_cache)\n",
    "            prediction = pred_cache[max_idx, :seq_pos+1]\n",
    "\n",
    "            # 현재 Branch의 예측 및 스코어 업데이트\n",
    "            pred_tmp[beam_idx, :seq_pos+1] = prediction\n",
    "            scores[beam_idx] = score_cache[max_idx]\n",
    "            score_cache[max_idx] = -1\n",
    "\n",
    "            # EOS 토큰이 예측된 경우 EOS 플래그 설정\n",
    "            if prediction[-1] == tgt_tokenizer.eos_id():\n",
    "                eos_flag[beam_idx] = -1\n",
    "\n",
    "    # 최종 예측 결과 정리\n",
    "    pred = []\n",
    "    for long_pred in pred_tmp:\n",
    "        zero_idx = long_pred.tolist().index(tgt_tokenizer.eos_id())\n",
    "        short_pred = long_pred[:zero_idx+1]\n",
    "        pred.append(short_pred)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "16395996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    \"\"\"\n",
    "    Calculate BLEU score between a reference and a candidate sentence.\n",
    "\n",
    "    Args:\n",
    "    - reference (list): A list containing the reference sentence as a list of tokens.\n",
    "    - candidate (list): A list containing the candidate sentence as a list of tokens.\n",
    "    - weights (list, optional): Weights for the n-gram scores (default is [0.25, 0.25, 0.25, 0.25]).\n",
    "\n",
    "    Returns:\n",
    "    - float: BLEU score between the reference and candidate sentences.\n",
    "    \"\"\"\n",
    "    return sentence_bleu([reference],\n",
    "                         candidate,\n",
    "                         weights=weights,\n",
    "                         smoothing_function=SmoothingFunction().method1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c9fccd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_bleu(reference, ids, tokenizer):\n",
    "    \"\"\"\n",
    "    빔 서치를 사용하여 생성된 일련의 후보 시퀀스에 대한 평균 BLEU 점수를 계산합니다.\n",
    "\n",
    "    Args:\n",
    "    - reference (str): 참조 문장으로 문자열입니다.\n",
    "    - ids (list): 각 후보 시퀀스의 토큰 ID를 포함하는 하위 목록이 있는 목록입니다.\n",
    "    - tokenizer: 토큰 ID를 디코딩하는 데 사용되는 토크나이저 객체입니다.\n",
    "\n",
    "    Returns:\n",
    "    - float: 후보 시퀀스에 대한 평균 BLEU 점수입니다.\n",
    "    \"\"\"\n",
    "    reference = reference.split()\n",
    "\n",
    "    total_score = 0.0\n",
    "    for _id in ids:\n",
    "        candidate = tokenizer.decode_ids(_id.tolist()).split()\n",
    "        score = calculate_bleu(reference, candidate)\n",
    "\n",
    "        print(\"Reference:\", reference)\n",
    "        print(\"Candidate:\", candidate)\n",
    "        print(\"BLEU:\", calculate_bleu(reference, candidate))\n",
    "\n",
    "        total_score += score\n",
    "        \n",
    "    return total_score / len(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "32eb39b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: ['tienes', 'que', 'decírmelo.']\n",
      "Candidate: ['tenés', 'que', 'decirlo', 'a', 'mí?']\n",
      "BLEU: 0.05372849659117709\n",
      "Reference: ['tienes', 'que', 'decírmelo.']\n",
      "Candidate: ['tienes', 'que', 'decirlo', 'a', 'mí?']\n",
      "BLEU: 0.11362193664674995\n",
      "Reference: ['tienes', 'que', 'decírmelo.']\n",
      "Candidate: ['tenés', 'que', 'decirlo', 'a', 'mí', 'mismo']\n",
      "BLEU: 0.040824829046386304\n",
      "Reference: ['tienes', 'que', 'decírmelo.']\n",
      "Candidate: ['tienes', 'que', 'decirlo', 'a', 'mí', 'mismo']\n",
      "BLEU: 0.08633400213704504\n",
      "Reference: ['tienes', 'que', 'decírmelo.']\n",
      "Candidate: ['tenés', 'que', 'decirlo', 'por', 'mí?']\n",
      "BLEU: 0.05372849659117709\n",
      "0.0696475522025071\n"
     ]
    }
   ],
   "source": [
    "test_idx = 1\n",
    "\n",
    "ids = \\\n",
    "beam_search_decoder(test_eng_sentences[test_idx],\n",
    "                    MAX_LEN,\n",
    "                    MAX_LEN,\n",
    "                    transformer,\n",
    "                    tokenizer,\n",
    "                    tokenizer,\n",
    "                    beam_size=5)\n",
    "\n",
    "bleu = beam_bleu(test_spa_sentences[test_idx], ids, tokenizer)\n",
    "print(bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdb4cba",
   "metadata": {},
   "source": [
    "# 데이터 부풀리기\n",
    "- Embedding을 활용한 Lexical Substitution 구현\n",
    "\n",
    "- gensim 에 사전 훈련된 Embedding 모델을 불러오는 것은 두 가지 방법이 있습니다.\n",
    "\n",
    "    1. 직접 모델을 다운로드해 load 하는 방법\n",
    "    2. gensim 이 자체적으로 지원하는 downloader 를 활용해 모델을 load 하는 방법  \n",
    "    \n",
    "    \n",
    "- 한국어는 gensim 에서 지원하지 않으므로 2번째 방법은 사용할 수 없음\n",
    "- 대표적으로 사용되는 Embedding 모델은 word2vec-google-news-300 이지만 용량이 커서 다운로드에 많은 시간이 소요\n",
    "- 이번에는 적당한 사이즈의 모델인 glove-wiki-gigaword-300 을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "34dfd4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 훈련된 Embedding 모델을 다운로드\n",
    "import gensim.downloader as api\n",
    "\n",
    "wv = api.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c77265e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bananas', 0.6691170930862427),\n",
       " ('mango', 0.5804104208946228),\n",
       " ('pineapple', 0.5492372512817383),\n",
       " ('coconut', 0.5462778806686401),\n",
       " ('papaya', 0.541056752204895),\n",
       " ('fruit', 0.52181077003479),\n",
       " ('growers', 0.4877638816833496),\n",
       " ('nut', 0.48399588465690613),\n",
       " ('peanut', 0.48062023520469666),\n",
       " ('potato', 0.48061180114746094)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(\"banana\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73819926",
   "metadata": {},
   "source": [
    "주어진 데이터를 토큰 단위로 분리한 후, 랜덤하게 하나를 선정하여 해당 토큰과 가장 유사한 단어를 찾아 대치하면 그것으로 Lexical Substitution은 완성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bd517629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: you know ? all you need is attention .\n",
      "To: you know ? all you need is attention , \n"
     ]
    }
   ],
   "source": [
    "sample_sentence = \"you know ? all you need is attention .\"\n",
    "sample_tokens = sample_sentence.split()\n",
    "\n",
    "selected_tok = random.choice(sample_tokens)\n",
    "\n",
    "result = \"\"\n",
    "for tok in sample_tokens:\n",
    "    if tok is selected_tok:\n",
    "        result += wv.most_similar(tok)[0][0] + \" \"\n",
    "\n",
    "    else:\n",
    "        result += tok + \" \"\n",
    "\n",
    "print(\"From:\", sample_sentence)\n",
    "print(\"To:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bc218098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def lexical_sub(sentence, embedding_model, top_n=5):\n",
    "    \"\"\"\n",
    "    주어진 문장에 대해 단어 임베딩을 기반으로 한 어휘 대체를 수행합니다.\n",
    "\n",
    "    Args:\n",
    "    - sentence (str): 어휘 대체를 수행할 입력 문장입니다.\n",
    "    - embedding_model: 사전 훈련된 단어 임베딩 모델 (예: Word2Vec, FastText).\n",
    "    - top_n (int, optional): 각 단어를 대체할 상위 후보의 수 (기본값은 5).\n",
    "\n",
    "    Returns:\n",
    "    - str: 어휘 대체 후의 증강된 문장입니다.\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    augmented_sentence = []\n",
    "\n",
    "    for word in words:\n",
    "        if word in embedding_model:\n",
    "            word_embedding = np.array([embedding_model[word]])\n",
    "            word_similarities = embedding_model.similar_by_vector(word_embedding.flatten(), topn=top_n)\n",
    "            top_candidates = [candidate for candidate, _ in word_similarities]\n",
    "\n",
    "            # 가장 유사한 후보로 단어를 대체합니다.\n",
    "            augmented_sentence.append(random.choice(top_candidates))\n",
    "        else:\n",
    "            augmented_sentence.append(word)\n",
    "\n",
    "    return ' '.join(augmented_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f39186",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_corpus = []\n",
    "\n",
    "for old_src in tqdm(train_eng_sentences): \n",
    "    new_src = lexical_sub(old_src, wv)\n",
    "    if new_src is not None: \n",
    "        new_corpus.append(new_src)\n",
    "    # Augmentation이 없더라도 원본 문장을 포함시킵니다\n",
    "    new_corpus.append(old_src)\n",
    "\n",
    "print(new_corpus[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4fb2af",
   "metadata": {},
   "source": [
    "### 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b8917710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: you know ? all you need is attention .\n",
      "Augmented: you what n't other ? must this focus ,\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "def lexical_sub(sentence, embedding_model, top_n=5):\n",
    "    words = sentence.split()\n",
    "    augmented_sentence = []\n",
    "\n",
    "    for word in words:\n",
    "        if word in embedding_model:\n",
    "            word_embedding = np.array([embedding_model[word]])\n",
    "            word_similarities = embedding_model.similar_by_vector(word_embedding.flatten(), topn=top_n)\n",
    "            top_candidates = [candidate for candidate, _ in word_similarities]\n",
    "\n",
    "            # 가장 유사한 후보로 단어를 대체합니다.\n",
    "            augmented_sentence.append(random.choice(top_candidates))\n",
    "        else:\n",
    "            augmented_sentence.append(word)\n",
    "\n",
    "    return ' '.join(augmented_sentence)\n",
    "\n",
    "# 예시 사용법\n",
    "sample_sentence = \"you know ? all you need is attention .\"\n",
    "augmented_sentence = lexical_sub(sample_sentence, wv)\n",
    "print(\"Original:\", sample_sentence)\n",
    "print(\"Augmented:\", augmented_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
