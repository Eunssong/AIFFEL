{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20bb7c15",
   "metadata": {},
   "source": [
    "[GLUE benchmark의 한국어 버전 KLUE benchmark](https://klue-benchmark.com/)\n",
    "\n",
    "model(klue/ber-base)를 활용하여 NSMC(Naver Sentiment Movie Corpus) task 진행\n",
    "\n",
    "모델 : [KLUE/Bert-base](https://huggingface.co/klue/bert-base)  \n",
    "데이터셋 : [NSMC](https://github.com/e9t/nsmc)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82760cd9",
   "metadata": {},
   "source": [
    "### Huggingface transformers 설치 및 환경 구성\n",
    "$ python -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))\"\n",
    "\n",
    "위의 코드가 잘 실행되지 않을 경우 터미널을 열고 아래 명령어를 입력하여 환경을 구성합니다.\n",
    "$ pip uninstall transformers -y  \n",
    "\n",
    "$ pip install transformers  \n",
    "\n",
    "$ mkdir -p transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73a8fbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "1.21.4\n",
      "4.11.3\n",
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import numpy\n",
    "import transformers\n",
    "import datasets\n",
    "\n",
    "print(tensorflow.__version__)\n",
    "print(numpy.__version__)\n",
    "print(transformers.__version__)\n",
    "print(datasets.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eaa8ba",
   "metadata": {},
   "source": [
    "# NSMC 데이터 분석 및 Huggingface dataset 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85ef1762",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset nsmc (/aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d986fb38c5483d92137e5bac3657db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset #  Hugging Face 라이브러리의 함수\n",
    "\n",
    "huggingface_nsmc_dataset = load_dataset('nsmc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "432d7eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'document', 'label'],\n",
      "        num_rows: 150000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'document', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(huggingface_nsmc_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb5d3658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='string', id=None),\n",
       " 'document': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(num_classes=2, names=['negative', 'positive'], names_file=None, id=None)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = huggingface_nsmc_dataset['train']\n",
    "cols = train.features\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4664a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label : 0\n",
      "document : 아 더빙.. 진짜 짜증나네요 목소리\n",
      "id : 9976970\n",
      "\n",
      "\n",
      "label : 1\n",
      "document : 흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\n",
      "id : 3819312\n",
      "\n",
      "\n",
      "label : 0\n",
      "document : 너무재밓었다그래서보는것을추천한다\n",
      "id : 10265843\n",
      "\n",
      "\n",
      "label : 0\n",
      "document : 교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정\n",
      "id : 9045019\n",
      "\n",
      "\n",
      "label : 1\n",
      "document : 사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다\n",
      "id : 6483659\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    for col_name, col_data in train[i].items():\n",
    "        print(col_name, \":\", col_data)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87e35f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 데이터셋을 pandas DataFrame으로 변환\n",
    "df = pd.DataFrame(train[:5])\n",
    "\n",
    "# DataFrame 출력\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd435fe",
   "metadata": {},
   "source": [
    "# klue/bert-base model 및 tokenizer 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ff34e5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "huggingface_tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
    "huggingface_model = AutoModelForSequenceClassification.from_pretrained('klue/bert-base', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d134a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이징\n",
    "def transform(data):\n",
    "    return huggingface_tokenizer(\n",
    "    data['document'],\n",
    "    truncation = True, # 문장 길이 조정\n",
    "    padding = 'max_length',\n",
    "        return_token_type_ids = False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ab3282e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-522b4d629f2b3643.arrow\n",
      "Loading cached processed dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-c93bfbbf5a65bda7.arrow\n"
     ]
    }
   ],
   "source": [
    "hf_dataset = huggingface_nsmc_dataset.map(transform, batched = True)\n",
    "\n",
    "# train, test\n",
    "original_hf_train = hf_dataset['train']\n",
    "hf_test = hf_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fd0fb2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'document', 'id', 'input_ids', 'label'],\n",
       "        num_rows: 150000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'document', 'id', 'input_ids', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e16c83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원하는 데이터 크기\n",
    "desired_dataset_size = 15000  # 원하는 데이터셋 크기로 수정\n",
    "\n",
    "# 원하는 크기에 해당하는 데이터만 선택\n",
    "reduced_dataset = original_hf_train.select(range(desired_dataset_size))\n",
    "\n",
    "# 학습에 사용할 데이터셋\n",
    "hf_dataset['train'] = reduced_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c20830e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'document', 'id', 'input_ids', 'label'],\n",
       "        num_rows: 10500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'document', 'id', 'input_ids', 'label'],\n",
       "        num_rows: 4500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_train_datasets = hf_dataset['train'].train_test_split(test_size=0.3) # huggingface train_test_split\n",
    "hf_train_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54d847ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_train_dataset = hf_train_datasets['train']\n",
    "hf_val_dataset = hf_train_datasets['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99f1915",
   "metadata": {},
   "source": [
    "#  tokenizer으로 데이터셋을 전처리하고, model 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fedb630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "output_dir = '/aiffel/aiffel/DeepGoing_NLP'\n",
    "\n",
    "training_argument = TrainingArguments(\n",
    "    output_dir, # output 저장될 경로\n",
    "    evaluation_strategy = 'epoch', # 평가하는 빈도\n",
    "    learning_rate = 2e-5, \n",
    "    per_device_train_batch_size = 8, # 각 device 당 batch size\n",
    "    per_device_eval_batch_size = 8, # evaluation 시에 batch size\n",
    "    num_train_epochs = 3,\n",
    "    weight_decay = 0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "073bd193",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "metric = load_metric('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis = 1)\n",
    "    return metric.compute(predictions=predictions, references = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "036ad189",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = huggingface_model, # 학습시킬 모델\n",
    "    args = training_argument, # training_argument\n",
    "    train_dataset = hf_train_dataset, # train_dataset\n",
    "    eval_dataset = hf_val_dataset,\n",
    "    compute_metrics = compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c8940e0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running training *****\n",
      "  Num examples = 10500\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3939\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3939' max='3939' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3939/3939 1:00:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.164600</td>\n",
       "      <td>0.583884</td>\n",
       "      <td>0.870667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.170700</td>\n",
       "      <td>0.608780</td>\n",
       "      <td>0.876667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.068300</td>\n",
       "      <td>0.731153</td>\n",
       "      <td>0.876444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /aiffel/aiffel/DeepGoing_NLP/checkpoint-500\n",
      "Configuration saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-500/config.json\n",
      "Model weights saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/DeepGoing_NLP/checkpoint-1000\n",
      "Configuration saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-1000/config.json\n",
      "Model weights saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-1000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/DeepGoing_NLP/checkpoint-1500\n",
      "Configuration saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-1500/config.json\n",
      "Model weights saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/DeepGoing_NLP/checkpoint-2000\n",
      "Configuration saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-2000/config.json\n",
      "Model weights saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/DeepGoing_NLP/checkpoint-2500\n",
      "Configuration saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-2500/config.json\n",
      "Model weights saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-2500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/DeepGoing_NLP/checkpoint-3000\n",
      "Configuration saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-3000/config.json\n",
      "Model weights saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/DeepGoing_NLP/checkpoint-3500\n",
      "Configuration saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-3500/config.json\n",
      "Model weights saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-3500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4500\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3939, training_loss=0.135897266710431, metrics={'train_runtime': 3613.4711, 'train_samples_per_second': 8.717, 'train_steps_per_second': 1.09, 'total_flos': 8287998243840000.0, 'train_loss': 0.135897266710431, 'epoch': 3.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c3e998a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 30:52]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7565054893493652, 'eval_accuracy': 0.87308, 'eval_runtime': 1852.7813, 'eval_samples_per_second': 26.986, 'eval_steps_per_second': 3.373, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "eval_metrics = trainer.evaluate(hf_test)\n",
    "print(eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d0a91a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del huggingface_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da00353",
   "metadata": {},
   "source": [
    "# Fine-tuning을 통하여 모델 성능(accuarcy) 향상시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "864fa65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "huggingface_model = AutoModelForSequenceClassification.from_pretrained('klue/bert-base', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "521625ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_argument = TrainingArguments(\n",
    "    output_dir, # output 저장될 경로\n",
    "    evaluation_strategy = 'epoch', # 평가하는 빈도\n",
    "    learning_rate = 5e-5, \n",
    "    per_device_train_batch_size = 8, # 각 device 당 batch size\n",
    "    per_device_eval_batch_size = 8, # evaluation 시에 batch size\n",
    "    num_train_epochs = 3,\n",
    "    weight_decay = 0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e3af526",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = huggingface_model, # 학습시킬 모델\n",
    "    args = training_argument, # training_argument\n",
    "    train_dataset = hf_train_dataset, # train_dataset\n",
    "    eval_dataset = hf_val_dataset,\n",
    "    compute_metrics = compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fda3be1f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running training *****\n",
      "  Num examples = 10500\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3939\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3939' max='3939' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3939/3939 1:01:17, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.404900</td>\n",
       "      <td>0.362438</td>\n",
       "      <td>0.863111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.272000</td>\n",
       "      <td>0.452394</td>\n",
       "      <td>0.868000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.130200</td>\n",
       "      <td>0.603608</td>\n",
       "      <td>0.874444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /aiffel/aiffel/DeepGoing_NLP/checkpoint-500\n",
      "Configuration saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-500/config.json\n",
      "Model weights saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/DeepGoing_NLP/checkpoint-1000\n",
      "Configuration saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-1000/config.json\n",
      "Model weights saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-1000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/DeepGoing_NLP/checkpoint-1500\n",
      "Configuration saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-1500/config.json\n",
      "Model weights saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/DeepGoing_NLP/checkpoint-2000\n",
      "Configuration saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-2000/config.json\n",
      "Model weights saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/DeepGoing_NLP/checkpoint-2500\n",
      "Configuration saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-2500/config.json\n",
      "Model weights saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-2500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/DeepGoing_NLP/checkpoint-3000\n",
      "Configuration saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-3000/config.json\n",
      "Model weights saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/DeepGoing_NLP/checkpoint-3500\n",
      "Configuration saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-3500/config.json\n",
      "Model weights saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-3500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4500\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3939, training_loss=0.2788356113748582, metrics={'train_runtime': 3678.2123, 'train_samples_per_second': 8.564, 'train_steps_per_second': 1.071, 'total_flos': 8287998243840000.0, 'train_loss': 0.2788356113748582, 'epoch': 3.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0e6de24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 30:50]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6371132731437683, 'eval_accuracy': 0.8694, 'eval_runtime': 1850.8266, 'eval_samples_per_second': 27.015, 'eval_steps_per_second': 3.377, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "eval_metrics = trainer.evaluate(hf_test)\n",
    "print(eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb7394cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "del huggingface_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6911617c",
   "metadata": {},
   "source": [
    "# Bucketing을 적용하여 학습\n",
    "[Data Collator](https://huggingface.co/docs/transformers/v4.30.0/en/main_classes/data_collator)\n",
    "\n",
    "[Trainer.TrainingArguments 의 group_by_length](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)\n",
    "\n",
    "STEP 4에 학습한 결과와 bucketing을 적용하여 학습시킨 결과를 비교해보고, 모델 성능 향상과 훈련 시간 두 가지 측면에서 각각 어떤 이점이 있는지 비교해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "897a5a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/vocab.txt from cache at /aiffel/.cache/huggingface/transformers/1a36e69d48a008e522b75e43693002ffc8b6e6df72de7c53412c23466ec165eb.085110015ec67fc02ad067f712a7c83aafefaf31586a3361dd800bcac635b456\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/tokenizer.json from cache at /aiffel/.cache/huggingface/transformers/310a974e892b181d75eed58b545cc0592d066ae4ef35cc760ea92e9b0bf65b3b.74f7933572f937b11a02b2cfb4e88a024059be36c84f53241b85b1fec49e21f7\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/special_tokens_map.json from cache at /aiffel/.cache/huggingface/transformers/aeaaa3afd086a040be912f92ffe7b5f85008b744624f4517c4216bcc32b51cf0.054ece8d16bd524c8a00f0e8a976c00d5de22a755ffb79e353ee2954d9289e26\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/tokenizer_config.json from cache at /aiffel/.cache/huggingface/transformers/f8f71eb411bb03f57b455cfb1b4e04ae124201312e67a3ad66e0a92d0c228325.78871951edcb66032caa0a9628d77b3557c23616c653dacdb7a1a8f33011a843\n",
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/klue/bert-base/resolve/main/pytorch_model.bin from cache at /aiffel/.cache/huggingface/transformers/05b36ee62545d769939a7746eca739b844a40a7a7553700f110b58b28ed6a949.7cb231256a5dbe886e12b902d05cb1241f330d8c19428508f91b2b28c1cfe0b6\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "huggingface_tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "huggingface_model = AutoModelForSequenceClassification.from_pretrained('klue/bert-base', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3da8bc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_argument = TrainingArguments(\n",
    "    output_dir, # output 저장될 경로\n",
    "    evaluation_strategy = 'epoch', # 평가하는 빈도\n",
    "    learning_rate = 2e-5, \n",
    "    per_device_train_batch_size = 8, # 각 device 당 batch size\n",
    "    per_device_eval_batch_size = 8, # evaluation 시에 batch size\n",
    "    num_train_epochs = 3,\n",
    "    weight_decay = 0.01, \n",
    "    group_by_length = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "845f8cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=huggingface_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e291bf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = huggingface_model, # 학습시킬 모델\n",
    "    args = training_argument, # training_argument\n",
    "    data_collator=data_collator, # DataCollatorForLanguageModeling\n",
    "    train_dataset = hf_train_dataset, # train_dataset\n",
    "    eval_dataset = hf_val_dataset,\n",
    "    compute_metrics = compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c61d7a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running training *****\n",
      "  Num examples = 10500\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3939\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3939' max='3939' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3939/3939 1:01:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.370100</td>\n",
       "      <td>0.324814</td>\n",
       "      <td>0.871111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.253600</td>\n",
       "      <td>0.459641</td>\n",
       "      <td>0.877778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.144200</td>\n",
       "      <td>0.588307</td>\n",
       "      <td>0.878222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /aiffel/aiffel/DeepGoing_NLP/checkpoint-500\n",
      "Configuration saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-500/config.json\n",
      "Model weights saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/DeepGoing_NLP/checkpoint-1000\n",
      "Configuration saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-1000/config.json\n",
      "Model weights saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-1000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/DeepGoing_NLP/checkpoint-1500\n",
      "Configuration saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-1500/config.json\n",
      "Model weights saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/DeepGoing_NLP/checkpoint-2000\n",
      "Configuration saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-2000/config.json\n",
      "Model weights saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/DeepGoing_NLP/checkpoint-2500\n",
      "Configuration saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-2500/config.json\n",
      "Model weights saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-2500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/DeepGoing_NLP/checkpoint-3000\n",
      "Configuration saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-3000/config.json\n",
      "Model weights saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/DeepGoing_NLP/checkpoint-3500\n",
      "Configuration saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-3500/config.json\n",
      "Model weights saved in /aiffel/aiffel/DeepGoing_NLP/checkpoint-3500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4500\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3939, training_loss=0.26408320763716997, metrics={'train_runtime': 3683.5687, 'train_samples_per_second': 8.551, 'train_steps_per_second': 1.069, 'total_flos': 8287998243840000.0, 'train_loss': 0.26408320763716997, 'epoch': 3.0})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5dff8fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 30:49]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.58770352602005, 'eval_accuracy': 0.87628, 'eval_runtime': 1849.559, 'eval_samples_per_second': 27.033, 'eval_steps_per_second': 3.379, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "eval_metrics = trainer.evaluate(hf_test)\n",
    "print(eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0719f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "del huggingface_model"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "b33d0f3f",
   "metadata": {},
   "source": [
    "# 회고\n",
    "- DataCollatorForLanguageModeling과 group_by_length = True를 설정하면서 huggingface 공식 홈페이지와 친해져야겠다는 것을 느꼈다\n",
    "- cuda out of memory 로 인해 데이터를 1/10로 줄여서 학습을 진행했더니 학습시에 Validation Loss가 증가해서 과적합이 일어난 것 같다. \n",
    "![image.png](attachment:image.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
